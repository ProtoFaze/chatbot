{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "\n",
    "# Example chat log\n",
    "chat_log = \"Hello there \\ud83d\\ude04! How are you \\ud83d\\ude0a?\"\n",
    "\n",
    "def decode_emojis(text):\n",
    "    # Match Unicode escape sequences (e.g., \\ud83d\\ude04)\n",
    "    emoji_pattern = re.compile(r'(\\\\u[\\da-fA-F]{4}\\\\u[\\da-fA-F]{4})')\n",
    "\n",
    "    def decode_match(match):\n",
    "        # Decode each matched emoji escape sequence\n",
    "        return match.group(0).encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "\n",
    "    # Replace all encoded emojis with actual emojis\n",
    "    return emoji_pattern.sub(decode_match, text)\n",
    "\n",
    "def demojize_chatlog(chat_log):\n",
    "    # Decode emojis\n",
    "    decoded_text = decode_emojis(chat_log)\n",
    "    # Convert emojis to text representations\n",
    "    return emoji.demojize(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def format_chat_message(chat_message):\n",
    "    chat_message = re.sub(r'[\\u202F\\u2011\\u202a\\u202c\\u200e]', ' ', chat_message)\n",
    "    chat_message = re.sub(r'[\\u2018\\u2019]', \"'\", chat_message)\n",
    "    # Extract the components of the chat message\n",
    "    if ']' in chat_message:\n",
    "        timestamp_str, rest = chat_message.strip().split('] ', 1)\n",
    "        timestamp_str = timestamp_str[1:]  # Remove the leading '['\n",
    "        sender, message = rest.split(': ', 1)\n",
    "    # Check if sender is a phone number or has the prefix \"cust\"\n",
    "        if re.match(r'\\+\\d{1,4}(?:\\s\\d+)+', sender.strip()) or sender.lower().strip().startswith('cust'):\n",
    "            sender = 'user'\n",
    "        else:\n",
    "            sender = 'agent'    \n",
    "\n",
    "        # Convert the timestamp to ISO 8601 format\n",
    "        try:\n",
    "            # Try 24-hour format first\n",
    "            timestamp = datetime.strptime(timestamp_str, '%d/%m/%Y, %H:%M:%S').isoformat()\n",
    "            # print(\"converting\"+timestamp_str+\" with 24 hour format\")\n",
    "        except ValueError:\n",
    "            # If it fails, try 12-hour format with AM/PM\n",
    "            # print(\"converting\"+timestamp_str+\" with 12 hour format\")\n",
    "            timestamp = datetime.strptime(timestamp_str, '%d/%m/%Y, %I:%M:%S %p').isoformat()\n",
    "        \n",
    "        # Create the object\n",
    "        if 'attached' in message and '.jpg' in message or \"image omitted\" in message:\n",
    "            return {\"timestamp\": timestamp,\"role\": sender.strip(),\"message\": \"<|image|>\"}\n",
    "        demojized_message = demojize_chatlog(message.strip())\n",
    "        return {\"timestamp\": timestamp,\"role\": sender.strip(),\"message\": demojized_message}\n",
    "    if 'attached' in chat_message and '.jpg' in chat_message or \"image omitted\" in chat_message:\n",
    "        return {\"role\": None,\"message\": \"<|image|>\"}\n",
    "    demojized_message = demojize_chatlog(chat_message.strip())\n",
    "    return {\"role\": None,\"message\": demojized_message}\n",
    "\n",
    "# Example usage\n",
    "# chat_message = \"[28/10/2024, 5:01:54â€¯PM] â€ª+60 12â€‘3456 7890â€¬: â€ŽMessages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them.\"\n",
    "# formatted_message = format_chat_message(chat_message)\n",
    "# print(formatted_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text files in data/chatlogs\n",
    "files = [\"_chat.txt\", \"_chat 2.txt\", \"_chat 3.txt\"]\n",
    "chatlogs = []\n",
    "for file in files:\n",
    "    previous_message = None\n",
    "    chatlog = []\n",
    "    with open(\"../data/chatlogs/\"+file, \"r\") as f:\n",
    "        file_messages = f.readlines()\n",
    "        last_message = demojize_chatlog(file_messages[-1].split(\": \")[1].strip())\n",
    "        for line in file_messages[1:]:\n",
    "            if \"uses a default timer for disappearing messages\" in line: #remove system message\n",
    "                continue\n",
    "            if (line.strip().strip(\"\\n\") == \"\" )&(previous_message != None): #in case there are multiline messages with empty lines\n",
    "                previous_message[\"message\"] += \" \\n \"\n",
    "                continue\n",
    "            message = format_chat_message(line)\n",
    "\n",
    "            # treat consecutive messages from the same user as one message\n",
    "            \n",
    "            if previous_message is None:\n",
    "                previous_message = message\n",
    "                continue\n",
    "\n",
    "            if message[\"role\"] == None: #concatenate next line of the multiline message# \n",
    "                previous_message[\"message\"] += message[\"message\"] \n",
    "                if last_message in previous_message[\"message\"]: chatlog.append(previous_message)\n",
    "            elif previous_message[\"role\"] == message[\"role\"]: # concatenate consecutive messages\n",
    "                previous_message[\"message\"] += \"<|USER_MSG|>\" + message[\"message\"] \n",
    "                if last_message in previous_message[\"message\"]: chatlog.append(previous_message)\n",
    "            else:\n",
    "                chatlog.append(previous_message)\n",
    "                if last_message in message[\"message\"]: chatlog.append(message)#detect if last message is detected\n",
    "                previous_message = message #switch users\n",
    "\n",
    "    chatlogs.append(chatlog)\n",
    "\n",
    "for chatlog in chatlogs:\n",
    "    for message in chatlog:\n",
    "        print(message['message'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chatlog in enumerate(chatlogs):\n",
    "    with open(f'../data/cleaned_chatlogs/chatlog{i}.json', 'w') as f:\n",
    "        f.write('[\\n')\n",
    "        for message in chatlog:\n",
    "            f.write(json.dumps(message))\n",
    "            f.write('\\n') if chatlog.index(message) == len(chatlog)-1 else f.write(',\\n')\n",
    "        f.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU ollama\n",
    "\n",
    "import ollama\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to send prompt to Ollama and get a response\n",
    "def mask_data(prompt_text: str, model=\"llama3.2\"):\n",
    "    response = ollama.generate(\n",
    "        model=model,\n",
    "        prompt=prompt_text,\n",
    "        stream=False,\n",
    "        system=\"\"\"You are a function that masks all personal information to protect customer privacy. Use:\n",
    "[NAME] for names, [LOCATION] for places, [DATE] for dates, [NUMBER] for numeric values.\n",
    "Input: \"{{text}}\"\n",
    "Output:\n",
    "<|start_header_id|>assistant<|end_header_id>\"\"\"\n",
    "    )\n",
    "    return response['response']\n",
    "\n",
    "# Function to anonymize personal info\n",
    "def anonymize_personal_info(data):\n",
    "    # Prepare prompts and execute in parallel\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        results = list(executor.map(mask_data, data))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q \"presidio_analyzer[transformers]\"\n",
    "# %pip install -q presidio_anonymizer\n",
    "# %%python -m spacy download en_core_web_sm\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.nlp_engine import TransformersNlpEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "\n",
    "text = \"<|AI_MSG|>Hi Mana , sorry for interrupting, I am Izzy Khan from Great North. I would like to ask if you are interested in subscribing to our life insurance, the monthly premium is only 45 ringgit.<|AI_MSG|>We also have a promotion for anyone who confirms subscribing today, we will give you a RM50 e-wallet. <|USER_MSG|>Sorry, I am not interested. I have subscribed to insurance. Thank you. <|AI_MSG|>Okay, thank you for responding. Can you also promote your friends or family members? <|USER_MSG|>My friend is an insurance agent ðŸ˜…\"\n",
    "# Define which transformers model to use\n",
    "model_config = [{\"lang_code\": \"en\", \"model_name\": {\n",
    "    \"spacy\": \"en_core_web_sm\",  # use a small spaCy model for lemmas, tokens etc.\n",
    "    \"transformers\": \"dslim/bert-base-NER\"\n",
    "    }\n",
    "}]\n",
    "\n",
    "nlp_engine = TransformersNlpEngine(models=model_config)\n",
    "\n",
    "# Set up the engine, loads the NLP module (spaCy model by default) \n",
    "# and other PII recognizers\n",
    "analyzer = AnalyzerEngine(nlp_engine=nlp_engine)\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "def anoynmize(text):\n",
    "    # Call analyzer to get results\n",
    "    results = analyzer.analyze(text=text, language='en')\n",
    "    # Analyzer results are passed to the AnonymizerEngine for anonymization\n",
    "    anonymized_text = anonymizer.anonymize(text=text, analyzer_results=results)\n",
    "\n",
    "    return anonymized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "import os\n",
    "directory_path = '../data/chatlogs_cleaned/'\n",
    "num_files = len([f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))])\n",
    "\n",
    "all_special_ids = [0, 1, 2]\n",
    "prefix = 'terjemah ke Melayu: '\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/nanot5-base-malaysian-translation-v2')\n",
    "model = T5ForConditionalGeneration.from_pretrained('mesolitica/nanot5-base-malaysian-translation-v2')\n",
    "\n",
    "def translate_text(text, max_length=512, prefix=prefix):\n",
    "    input_ids = tokenizer.encode(f'{prefix}{text}{tokenizer.eos_token}', return_tensors='pt')\n",
    "    outputs = model.generate(input_ids, max_length=max_length)\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "for i in range(num_files-1):\n",
    "    with open(f'../data/chatlogs_cleaned/chatlog{i}.json', 'r') as freader:\n",
    "        # Load the JSON arrays\n",
    "        messages_json = None\n",
    "        chatlog = freader.read()        \n",
    "        try:\n",
    "            messages_json = json.loads(chatlog)\n",
    "            messages = [message['message'].replace(\"\\n\",\"\") for message in messages_json]\n",
    "            \n",
    "            # print(f\"before processing\\n{messages}\")\n",
    "            malay_translated_messages = [translate_text(chunk) for chunk in messages]\n",
    "            # print(f\"translated to malay\\n{malay_translated_messages}\")\n",
    "            english_translated_messages = [translate_text(message, prefix=\"terjemah ke Inggeris: \") for message in malay_translated_messages]\n",
    "            # print(f\"translated to english\\n{english_translated_messages}\")\n",
    "            anonymized_messages = [anoynmize(message).text for message in english_translated_messages]\n",
    "            print(f\"anonymized\\n{anonymized_messages}\")\n",
    "            \n",
    "            anonymized_string = ' <|AI_MSG|> '.join(message for message in anonymized_messages)\n",
    "            anonymized_messages = anonymized_string.split('<|AI_MSG|>')\n",
    "            for index, message in enumerate(messages_json):\n",
    "                messages_json[index]['translated_message'] = anonymized_messages[index].strip()\n",
    "                print(anonymized_messages[index].strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chatlog{i}.json: {e.with_traceback()}\")\n",
    "    with open(f'../data/chatlogs_translated/chatlog{i}.json', 'w') as fwriter:\n",
    "        fwriter.write('[\\n')\n",
    "        for index, message in enumerate(messages_json):\n",
    "            fwriter.write(json.dumps(messages_json[index]))\n",
    "            fwriter.write('\\n') if messages_json.index(message) == len(messages_json)-1 else fwriter.write(',\\n')\n",
    "        fwriter.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "with open(f'../data/chatlogs_translated/chatlog1.json', \"r\") as reader:\n",
    "    chatlog = reader.read()      \n",
    "    chatson = json.loads(chatlog)\n",
    "    results = analyzer.analyze(text=chatson[0]['translated_message'], language='en')\n",
    "    print(results)\n",
    "    for msgs in chatson:\n",
    "        res = ollama.generate(\n",
    "            model=\"llama3.2\",\n",
    "            prompt= f\"use <PERSON> to anonymize the names from the text: '{msgs['translated_message']}'\",\n",
    "            stream=False\n",
    "        )\n",
    "        # chatson[0]['translated_message'] =  anonymized_text = anonymizer.anonymize(text=chatson[0]['translated_message'], analyzer_results={})\n",
    "        print(res['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
