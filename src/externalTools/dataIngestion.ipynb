{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU llama-index llama-parse llama-index-embeddings-ollama llama_index-llms-ollama llama-index-vector-stores-mongodb --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "from pymongo import MongoClient\n",
    "mongo_client = MongoClient(os.environ[\"MONGODB_URI\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data aquisition:   \n",
    "Fetch all relevant documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **If you directly use a pdf as data**:\n",
    "- You need access to llama index \n",
    "    - Specifically u need a llama-cloud account as well as your own llama-cloud api key\n",
    "- Once you have your own API key\n",
    "    - You can save it in a `.env` file according to `.env.example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llamaparse_api_key = os.getenv(\"LLAMA_PARSE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup llama parse from llama index\n",
    "- A freemium document parser/data converter that can help parse **66** pages per day FOR FREE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from llama_parse import LlamaParse  # pip install llama-parse\n",
    "from llama_index.core import SimpleDirectoryReader, Document  # pip install llama-index\n",
    "\n",
    "results_encoding = 'markdown'\n",
    "parser = LlamaParse(\n",
    "    api_key=llamaparse_api_key,\n",
    "    result_type=results_encoding,\n",
    "    premium_mode=True,\n",
    "    disable_image_extraction=True,\n",
    "    take_screenshot=False,\n",
    "    parsing_instruction= \"This is an insurance document. SOME tables have structural issues like headers as columns or multi-line headers. RESTRUCTURE those tables, leave the rest as is.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you had already parsed your document in llamaCloud's online Dashboard or with python\n",
    "Get the job id and results_type for specifiying the encoding for parsing the given document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get history of parsing requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = None\n",
    "async with parser.client_context() as client:\n",
    "    headers = {\"Authorization\": f\"Bearer {parser.api_key}\"}\n",
    "    history = await client.get(\"https://api.cloud.llamaindex.ai//api/v1/parsing/history\", headers=headers)\n",
    "    history = history.json()\n",
    "print(\"usable records:\")\n",
    "for record in history:\n",
    "    if record['expired'] == False: print(f\"day: {record['day']} \\t job: {record['job_id']}\")\n",
    "print(\"\\nexpired records (records that are automatically deleted after 2 days in llama-parse): \")\n",
    "for record in history[:3]:\n",
    "    if record['expired'] == True: print(f\"day: {record['day']} \\t job: {record['job_id']}\")\n",
    "print('\\nolder records ommited')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save it for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with parser.client_context() as client:\n",
    "    headers = {\"Authorization\": f\"Bearer {parser.api_key}\"}\n",
    "    history = await client.get(\"https://api.cloud.llamaindex.ai//api/v1/parsing/history\", headers=headers)\n",
    "    history = history.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the fresh (usable) data from the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = 'e6301071-cf33-444a-b447-b11d2d4e5e40' # replace with the job_id you want to get the result for\n",
    "response = await parser._get_job_result(job_id=job_id,result_type=results_encoding)\n",
    "if results_encoding == 'json':\n",
    "    results = response['pages']\n",
    "    for i, pages in enumerate(response['pages']):\n",
    "        with open(f'./outputs/brochure_{i}.json', 'w') as f:\n",
    "            f.write(json.dumps(pages['items']))\n",
    "else:\n",
    "    results = response['markdown']\n",
    "    with open(\"../data/outputs/data.md\", 'w') as f:\n",
    "        f.write(response['markdown'])\n",
    "    markdowns = response['markdown'].split('\\n---\\n')\n",
    "    documents = [Document(text=markdown, metadata = response['job_metadata']) for markdown in markdowns]\n",
    "for document in documents:\n",
    "    print(document.get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or you can parse a document that you have via the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=['../data/sources/BROCHURE.pdf'], file_extractor=file_extractor)\n",
    "documents = await reader.aload_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: MongoDB only\n",
    "\n",
    "[mongodb pipeline setup](https://medium.com/@abdulsomad.me/how-to-build-rag-app-with-mongodb-atlas-database-llama-index-gemini-llm-and-embedding-and-8e82df16d6bf)   \n",
    "the cheapest but most limited option\n",
    "\n",
    "as for setup, we need\n",
    "1. Mongodb client\n",
    "    - for setting up a client connection to the storage or project cluster\n",
    "    - uses a cluster-specific connection string provided in mongodb Atlas\n",
    "2. A storage context\n",
    "    - A llamaindex container to prepare data for storage\n",
    "3. Embedding model\n",
    "    - Can be from any provider `(openAI)`, self-hosted`(ollama)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "embed_model = OllamaEmbedding(model_name='nomic-embed-text', ollama_additional_kwargs={\"mirostat\": 0})\n",
    "llm = Ollama(model='llama3.2', request_timeout=60.0)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = None\n",
    "collection_name = \"llamaIndexChunk\"\n",
    "if collection_name in mongo_client[\"product1Chunked\"].list_collection_names():\n",
    "    print('getting collection')\n",
    "    collection = mongo_client[\"product1Chunked\"][collection_name]\n",
    "else:\n",
    "    print('creating collection')\n",
    "    collection = mongo_client[\"product1Chunked\"].create_collection('llamaIndexChunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the collection for which to create the index\n",
    "from pymongo.operations import SearchIndexModel\n",
    "\n",
    "index_name = 'vector_index'\n",
    "if index_name not in ([index['name'] for index in list(collection.list_search_indexes())]):\n",
    "  # Create your index model, then create the search index\n",
    "  search_index_model = SearchIndexModel(\n",
    "    definition={\n",
    "      \"fields\": [\n",
    "        {\n",
    "          \"type\": \"vector\",\n",
    "          \"path\": \"embedding\",\n",
    "          \"numDimensions\": 768,\n",
    "          \"similarity\": \"cosine\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"filter\",\n",
    "          \"path\": \"metadata.page_label\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    name=index_name,\n",
    "    type=\"vectorSearch\",\n",
    "  )\n",
    "  collection.create_search_index(model=search_index_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(collection.list_search_indexes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# Instantiate the vector store\n",
    "atlas_vector_store = MongoDBAtlasVectorSearch(\n",
    "    mongodb_client=mongo_client,\n",
    "    db_name = \"product1Chunked\",\n",
    "    collection_name = \"llamaIndexChunk\",\n",
    "    vector_index_name = index_name\n",
    ")\n",
    "vector_store_context = StorageContext.from_defaults(vector_store=atlas_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reset the index if it is not empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from tqdm import tqdm\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "buffer_size=1, breakpoint_percentile_threshold=98, embed_model=embed_model)\n",
    "nodes = semantic_splitter.get_nodes_from_documents(documents)\n",
    "# Progress bar\n",
    "pbar = tqdm(total=len(nodes), desc=\"Embedding Progress\", unit=\"node\")\n",
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=MetadataMode.EMBED)\n",
    "    )\n",
    "    node.embedding = node_embedding\n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_vector_store.add(nodes=nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a vector store and Store your data into it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data inference\n",
    "To query our data, we need to configure our storage and indexes into objects that can be used to infer our data,\n",
    "for example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "# Instantiate VectorStoreIndex object from your vector_store object\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store=atlas_vector_store)\n",
    "\n",
    "# Grab 5 search results\n",
    "retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=5)\n",
    "\n",
    "question = \"is the product shariah compliant\"\n",
    "# Query vector DB\n",
    "answer = retriever.retrieve(question)\n",
    "\n",
    "# Inspect results\n",
    "for i in answer:\n",
    "    print(i.text)\n",
    "    print(\"\\n ------------- NEW NODE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import get_response_synthesizer\n",
    "synth = get_response_synthesizer(streaming=True)\n",
    "# Pass in your retriever from above, which is configured to return the top 5 results\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever,response_synthesizer=synth)\n",
    "\n",
    "# Now you query:\n",
    "llm_query = query_engine.query('what are the covered illnesses')\n",
    "\n",
    "# Response:\n",
    "llm_query.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: MongoDB only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fetch the vector store as an index for semantic search\n",
    "then instantiate it as a tool for tool calling later used by our base language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core import get_response_synthesizer\n",
    "synth = get_response_synthesizer(streaming=True)\n",
    "index = VectorStoreIndex.from_vector_store(atlas_vector_store)\n",
    "query_engine = index.as_query_engine(similarity_top_k=5, llm=llm, response_synthesizer=synth)\n",
    "\n",
    "query_engine_tool = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"knowledge_base\",\n",
    "        description=(\n",
    "            \"Provides information about Group Multiple Benefits Insurance Scheme (GMBIS).\"\n",
    "            \"Use a detailed plain text question as input to the tool.\"\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.query(\"What are the covered illnesses\").print_response_stream()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
