{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# %pip install -U python-dotenv langchain langchain_community langchain_mongodb nltk langchain-ollama langchain-pinecone pinecone-notebooks einops langgraph langchain_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assuming you have a JSON copy of the data   \n",
    "Data aquisition:   \n",
    "import the json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# load a json file as data\n",
    "data  = json.loads(Path(\"../data/data.json\").read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data ingestion:   \n",
    "since it is better to supply the full context of structured and unstructured data fetched in the future,   \n",
    "we will store the raw text in mongodb first   \n",
    "and apply preprocessing later when we need semantically/meaning accurate search results (important for vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to your MongoDB Atlas(Cloud) cluster\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient(os.environ[\"MONGODB_URI\"])\n",
    "db = client[os.environ[\"MONGODB_DB\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_targets already exists in the database, skipping insertion\n",
      "insertion complete\n"
     ]
    }
   ],
   "source": [
    "#specify outer level json objects as mongodb collection names, then based on the specified names, create new collections and insert corresponding data\n",
    "\n",
    "#specify all the collections\n",
    "collection_names = []\n",
    "for key, value in data.items():\n",
    "    collection_names.append(key)\n",
    "    # if type(value) != str:\n",
    "    #     value = json.dumps(value)\n",
    "    # print(key+\":    \"+value)\n",
    "    \n",
    "#fetch existing collections\n",
    "ignored_and_existing_collections = db.list_collection_names()\n",
    "\n",
    "#specify the collections to ignore\n",
    "if \"questions_and_answers2\" not in ignored_and_existing_collections:\n",
    "        ignored_and_existing_collections.append(\"questions_and_answers2\")\n",
    "if \"exclusions2\" not in ignored_and_existing_collections:\n",
    "        ignored_and_existing_collections.append(\"exclusions2\")\n",
    "\n",
    "for collection_name in collection_names:\n",
    "    if collection_name in ignored_and_existing_collections:\n",
    "       continue\n",
    "    collection = db[collection_name]\n",
    "    if isinstance(data[collection_name], list):\n",
    "        print(f\"\"\"\n",
    "inserting many records of {data[collection_name]} into {collection_name} collection\n",
    "\"\"\")\n",
    "        collection.insert_many(data[collection_name])\n",
    "    else:\n",
    "        print(f\"\"\"\n",
    "inserting one record of {data[collection_name]} into {collection_name} collection\n",
    "\"\"\")\n",
    "        collection.insert_one(data[collection_name])\n",
    "\n",
    "# separately insert the vector targets\n",
    "collection = db[\"vector_targets\"]\n",
    "vector_target_records = list(collection.find())\n",
    "if len(vector_target_records) == 0:\n",
    "        print(f\"\"\"\n",
    "inserting one record of {data[\"vector_targets\"]} into {collection_name} collection\n",
    "\"\"\")\n",
    "        collection.insert_many(data[\"vector_targets\"])        \n",
    "else:\n",
    "    print(\"vector_targets already exists in the database, skipping insertion\")\n",
    "\n",
    "print(\"insertion complete\")\n",
    "\n",
    "ignored_and_existing_collections = None\n",
    "del ignored_and_existing_collections\n",
    "collection_names = None\n",
    "del collection_names\n",
    "vector_target_records = None\n",
    "del vector_target_records\n",
    "data = None\n",
    "del data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further preprocessing:   \n",
    "lowercasing   \n",
    "special character removal\n",
    "\n",
    "stop words removal\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/damonng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#use nltk to lowercase, remove special characters, tokenize, remove stopwords, and lemmatize the values in the json data\n",
    "#fetch all data cleaning resources\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#initialize the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#allow specific stopwords\n",
    "stop_words.remove(\"both\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    pattern = re.compile(r\"[;@#&*!()\\[\\]]\")\n",
    "    def get_wordnet_pos(tag):\n",
    "        match tag[0]:\n",
    "            case 'J':\n",
    "                return wordnet.ADJ\n",
    "            case 'V':\n",
    "                return wordnet.VERB\n",
    "            case 'R':\n",
    "                return wordnet.ADV\n",
    "            case _:\n",
    "                return wordnet.NOUN\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # Remove stop words and lemmatize\n",
    "    processed_tokens = [\n",
    "        lemmatizer.lemmatize(word.lower(), get_wordnet_pos(tag)) \n",
    "        for word, tag in pos_tags \n",
    "        if word.lower() not in stop_words and not pattern.match(word)\n",
    "    ]\n",
    "    # Join tokens back to a string\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "#setup a function to recursively preprocess the json data\n",
    "def traverse_json(data, json_text_action, target_key=None):\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, str) and (target_key is None or key == target_key):\n",
    "                # Preprocess the string value\n",
    "                data[key] = json_text_action(value)\n",
    "            elif isinstance(value, dict) or isinstance(value, list):\n",
    "                # Recursively preprocess the dictionary or list\n",
    "                traverse_json(value, json_text_action)\n",
    "    elif isinstance(data, list):\n",
    "        for i, element in enumerate(data):\n",
    "            if isinstance(element, str):\n",
    "                # Preprocess the string in the list and save it in the original position\n",
    "                data[i] = json_text_action(element)\n",
    "            elif isinstance(element, dict) or isinstance(element, list):\n",
    "                # Recursively preprocess the dictionary or list\n",
    "                traverse_json(element, json_text_action)\n",
    "    return data\n",
    "\n",
    "# preprocess_text(\"i need information on performing funds. how much my premium is allocated? the allocation rate, the pricing plan, how much will i be paying per premium payment, what is the coverage performance, what are the fundings, testing the word post-apocalyptic, 80 90% 20$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/KINGSTON SNV2S1000G Media/prototypes/python/chatbot/.venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Connect to your pinecone vector database and specify the working index\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone(os.environ[\"PINECONE_API_KEY\"])\n",
    "index = pc.Index(os.environ[\"PINECONE_INDEX_NAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data embedding//vectorization\n",
    "\n",
    "target descriptive values and generate embeddings for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model (https://huggingface.co/nomic-ai/nomic-embed-text-v1.5\")\n",
    "# %pip install sentence-transformers scipy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "global_embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "\n",
    "number_of_dimensions = 768 #the embedding dimensions according to the model documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the target fields to embed\n",
    "from copy import deepcopy\n",
    "\n",
    "def vectorize(texts):\n",
    "    \"vectorize the provided texts using a globally declared model.\"    \n",
    "    vectors = global_embedding_model.encode(texts)\n",
    "    return vectors\n",
    "\n",
    "ignore_collections = [\"vector_targets\", \"premium_plans\",\"important_notice\",\"total_investment_estimations\",\"premium_allocations\",\"test2\", \"test\",\"questions_and_answers2\",\"exclusions2\"]\n",
    "\n",
    "def vectorize_and_index(mongo_database=db, vector_index=index, ignore_collections=ignore_collections, debug=False):\n",
    "    \"\"\"vectorize the target fields and store the embeddings in the pinecone index.\n",
    "    \n",
    "current implmentation does not support converting mongo collection name to pinecone namespaces.\"\"\"\n",
    "    if mongo_database == None:\n",
    "        raise ValueError(\"mongo_database is required, the mongo database should also have a vector_targets collection specifying the target fields to vectorize\")\n",
    "    if not vector_index:\n",
    "        raise ValueError(\"vector_index is required\")\n",
    "    vectors = []\n",
    "    for collection_name in mongo_database.list_collection_names():\n",
    "        if collection_name in ignore_collections:\n",
    "            continue                                            #skip collections that should not to be vectorized\n",
    "        if debug:\n",
    "            print(f\"In {collection_name}\")\n",
    "        results = list(mongo_database[collection_name].find())\n",
    "        targets = list(mongo_database.vector_targets.find({\"name\": collection_name}))\n",
    "        for record in results:\n",
    "            if targets:\n",
    "                for target in targets:                              #iterate through the target fields from the same collection\n",
    "                    old_data = deepcopy(record[target['field']])              #store original info\n",
    "                    traverse_json(record, preprocess_text, target['field'])\n",
    "                    cleaned_data = record[target['field']]                     #clean the data\n",
    "                    vector = vectorize(record[target['field']])                #store the vectorized embeddings\n",
    "                    if debug:\n",
    "                        print(f\"original: {old_data}\\ncleaned : {cleaned_data}\\nlength  : {len(vector)}\\nvector  : {vector}\")\n",
    "                    if len(vector) == number_of_dimensions: #check if the vectorized embeddings are of the correct length\n",
    "                        vectors.append({\"id\": str(record['_id'])+\"-\"+collection_name+\"-\"+target['field'], \n",
    "                                        \"values\": vector\n",
    "                                        ,\"metadata\": {\"original\":cleaned_data}\n",
    "                                        })\n",
    "                        pass\n",
    "                    else:                                           \n",
    "                        #if the vectorized embeddings length not correct, possibility of other datatypes, iterate through the iterable\n",
    "                        for j , sub_vector in enumerate(vector):\n",
    "                            if debug:\n",
    "                                print(len(sub_vector),cleaned_data[j])\n",
    "                            vectors.append({\"id\": str(record['_id'])+\"-\"+collection_name+\"-\"+target['field']+\"-\"+str(j), \n",
    "                                            \"values\": sub_vector\n",
    "                                            ,\"metadata\": {\"original\":cleaned_data[j]}\n",
    "                                            })\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(\"no matching target, skipping vectorization\")\n",
    "                continue\n",
    "            \n",
    "    # vector_index.upsert(vectors=vectors)\n",
    "\n",
    "# vectorize_and_index(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using local(ollama) base model as chatbot model\n",
    "from langchain_ollama.chat_models import ChatOllama as Ollama\n",
    "intent_classifier = Ollama(model='llama3.2',#:1B\n",
    "                           temperature=0.1,\n",
    "                           num_predict=4,\n",
    "                           request_timeout=10,\n",
    "                           verbose=False )\n",
    "chatbot_model = Ollama(model='llama3.2', request_timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'read_units': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'matches': [{'id': '67171ded2ca774a8bfd83375-questions_and_answers-answer-2',\n",
       "               'score': 0.499227405,\n",
       "               'values': []},\n",
       "              {'id': '67171ded2ca774a8bfd83372-questions_and_answers-answer',\n",
       "               'score': 0.469530821,\n",
       "               'values': []},\n",
       "              {'id': '67171dec2ca774a8bfd8334d-advantages-advantage',\n",
       "               'score': 0.464928329,\n",
       "               'values': []},\n",
       "              {'id': '6717364e2ca774a8bfd83399-funds-description-2',\n",
       "               'score': 0.463731647,\n",
       "               'values': []},\n",
       "              {'id': '67171dec2ca774a8bfd8334b-critical_illnesses-disclaimer',\n",
       "               'score': 0.460437477,\n",
       "               'values': []},\n",
       "              {'id': '67171ded2ca774a8bfd83352-requirements-description',\n",
       "               'score': 0.454422921,\n",
       "               'values': []}],\n",
       "  'namespace': '',\n",
       "  'usage': {'read_units': 5}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup the semantic search function\n",
    "def semantic_search(question, model=global_embedding_model, index=index, top_k=6, verbose=False):\n",
    "    \"\"\"vectorizes the question and queries the pinecone index for the top 3 closest matches.\n",
    "\n",
    "current implementation does not support querying multiple namespaces or using mongoDB indexes.\n",
    "\n",
    "Args:\n",
    "    question (str): question string used to query the pinecone index\n",
    "    verbose (bool, optional): flag to turn on debug mode. Defaults to False.\n",
    "    model (SentenceTransformer, optional): model used for vectorizing a cleaned question. Defaults to global_embedding_model.\n",
    "    index (Pinecone.Index, optional): the provided pinecone index. Defaults to index.\n",
    "\n",
    "Returns:\n",
    "    list(QueryResponse): the top closest query results from the pinecone index\n",
    "    \"\"\"\n",
    "    if not index:\n",
    "        print(\"Index not found, please specify your pinecone or mongo search index\")\n",
    "        return\n",
    "    if not model:\n",
    "        print(\"Model not found\")\n",
    "        return\n",
    "    if verbose:\n",
    "        print(\"Encoding question...\")\n",
    "        \n",
    "    vectors = model.encode(question)\n",
    "    \n",
    "    if not isinstance(vectors, list): # Ensure vectors is a list of floats\n",
    "        vectors = vectors.tolist()\n",
    "    matches = []\n",
    "    spaces = index.describe_index_stats()['namespaces']\n",
    "    for key, value in spaces.items():\n",
    "        res = index.query(\n",
    "            namespace=key,\n",
    "            vector=vectors,\n",
    "            top_k=top_k,\n",
    "            include_metadata=False,\n",
    "            include_values=False\n",
    "        )\n",
    "        matches.append(res)\n",
    "        print(res['usage'])\n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "response = semantic_search(\"who manages this product\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting collection matches...\n",
      "no pinecone namespace found, checking id or additional metadata\n",
      "{'id': '67171ded2ca774a8bfd83375-questions_and_answers-answer-2',\n",
      " 'score': 0.499227405,\n",
      " 'values': []}\n",
      "{'id': '67171ded2ca774a8bfd83372-questions_and_answers-answer',\n",
      " 'score': 0.469530821,\n",
      " 'values': []}\n",
      "{'id': '67171dec2ca774a8bfd8334d-advantages-advantage',\n",
      " 'score': 0.464928329,\n",
      " 'values': []}\n",
      "{'id': '6717364e2ca774a8bfd83399-funds-description-2',\n",
      " 'score': 0.463731647,\n",
      " 'values': []}\n",
      "{'id': '67171dec2ca774a8bfd8334b-critical_illnesses-disclaimer',\n",
      " 'score': 0.460437477,\n",
      " 'values': []}\n",
      "{'id': '67171ded2ca774a8bfd83352-requirements-description',\n",
      " 'score': 0.454422921,\n",
      " 'values': []}\n"
     ]
    }
   ],
   "source": [
    "#setup a function to receive semantic search results and return the collection name and ids\n",
    "def get_collection_matches(response : list, verbose=False) -> list:\n",
    "    \"\"\"based on the response from a pinecone semantic search, extract and consolidate the collection name and ids of the matching documents.\n",
    "\n",
    "    Args:\n",
    "        response (list): a list of QueryResponse objects from a pinecone semantic search\n",
    "        verbose (bool, optional): flag to use debug mode. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: list of mongodb documents\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Getting collection matches...\")\n",
    "    document_metadata = []\n",
    "    for collection_matches in response:\n",
    "        collection = collection_matches['namespace']\n",
    "        is_using_default_namespace = False\n",
    "        if collection == \"\":\n",
    "            is_using_default_namespace = True\n",
    "            if verbose:\n",
    "                print(\"no pinecone namespace found, checking id or additional metadata\")\n",
    "        ids = []\n",
    "        for match in collection_matches['matches']:\n",
    "            if verbose:\n",
    "                print(match)\n",
    "            data_from_id = match['id'].split(\"-\")\n",
    "            match_id = data_from_id[0]\n",
    "            if is_using_default_namespace:\n",
    "                collection = data_from_id[1]\n",
    "            #filter out duplicate ids\n",
    "            if match_id not in ids:\n",
    "                ids.append(match_id)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"Duplicate id {match_id} for {collection} found in fetch list, ignoring\")\n",
    "                continue\n",
    "            if is_using_default_namespace:\n",
    "                document_metadata.append({'collection':collection, 'id':match_id})\n",
    "        if not is_using_default_namespace:\n",
    "            document_metadata.append({'collection':collection, 'ids':ids})\n",
    "        \n",
    "    return document_metadata\n",
    "\n",
    "document_metadata = get_collection_matches(response, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('67171ded2ca774a8bfd83375'),\n",
       "  'question': 'What are the current fees and charges?',\n",
       "  'answer': ['Insurance charges are Applicable to the sum assured, and vary depending on the average age profile and claim experience of the scheme.',\n",
       "   'Monthly Policy Fee is RM5.00',\n",
       "   'Fund Management Charge is 0.50% per annum. Note: The fees and charges levied may change from time to time.']},\n",
       " {'_id': ObjectId('67171ded2ca774a8bfd83372'),\n",
       "  'question': 'Can the Assured Member/spouse/children apply to contribute more',\n",
       "  'answer': 'Yes. The Assured Member/spouse/children will be required to reapply by completing a standard Group Proposal form, subject to approval by the Company and up to the maximum benefit allowed'},\n",
       " {'_id': ObjectId('67171dec2ca774a8bfd8334d'),\n",
       "  'advantage': 'Automatic premium remittance via credit card / bank deduction / JomPay ensures continuous protection.'},\n",
       " {'_id': ObjectId('6717364e2ca774a8bfd83399'),\n",
       "  'name': 'Dana Gemilang',\n",
       "  'description': ['The Dana Gemilang is a fund where 80% to 100% of the investments are in equities, which may be volatile in the short term.',\n",
       "   \"This fund seeks to achieve medium to long-term capital appreciation. Although the fund invests mainly in Malaysia (50% to 100%), it may also partially invest in Singapore (up to 25%) and Hong Kong (up to 25%), if and when necessary, to enhance the fund's returns.\",\n",
       "   'The fund only invests in Shariah-approved securities. Although this fund invests in Shariah-approved securities, the investment-linked insurance plan that utilises this fund is not a Shariah-compliant product.'],\n",
       "  'fund_performance': [{'year': 2017, 'return_percentage': 13.95},\n",
       "   {'year': 2018, 'return_percentage': -17.84},\n",
       "   {'year': 2019, 'return_percentage': 4.14},\n",
       "   {'year': 2020, 'return_percentage': 1.77},\n",
       "   {'year': 2021, 'return_percentage': -2.52}]},\n",
       " {'_id': ObjectId('67171dec2ca774a8bfd8334b'),\n",
       "  'disclaimer': 'Upon payment of claim for Angioplasty and other invasive treatments for coronary artery disease, the sum assured will be reduced by the quantum of the payment for Angioplasty and other invasive treatments for coronary artery disease. However, the premium shall remain unchanged. This benefit is subject to a limit of RM1,000,000 under the Policy and all other non-credit-related group policies (including supplementary contract and endorsement, if any) issued by the Company by any name or description which provide Critical Illness benefits or similar benefits on the same Life Assured. Complete definition of Critical Illness as mentioned in the Master Policy need to be fulfilled before any 45 Critical Illnesses claim can become payable.',\n",
       "  'illnesses': ['Heart Attack - of specified severity',\n",
       "   'Stroke - resulting in permanent neurological deficit',\n",
       "   'Coronary Artery By-pass Surgery',\n",
       "   'Cancer - of specified severity',\n",
       "   'Kidney Failure - requiring dialysis or transplant',\n",
       "   'Fulminant Viral Hepatitis',\n",
       "   'Major Organ/Bone Marrow Transplant',\n",
       "   'Paralysis of Limbs',\n",
       "   'Multiple Sclerosis',\n",
       "   'Primary Pulmonary Arterial Hypertension',\n",
       "   'Blindness - Permanent and Irreversible',\n",
       "   'Heart Valve Surgery',\n",
       "   'Deafness - Permanent and Irreversible',\n",
       "   'Surgery to Aorta',\n",
       "   'Loss of Speech',\n",
       "   \"Alzheimer's Disease / Severe Dementia\",\n",
       "   'Third Degree Burns - of specified severity',\n",
       "   'Coma - resulting in permanent neurological deficit',\n",
       "   'Cardiomyopathy - of specified severity',\n",
       "   'Motor Neuron Disease',\n",
       "   'HIV Infection due to Blood Transfusion',\n",
       "   \"Parkinson's Disease\",\n",
       "   'End-Stage Liver Failure',\n",
       "   'End-Stage Lung Disease',\n",
       "   'Major Head Trauma',\n",
       "   'Chronic Aplastic Anemia',\n",
       "   'Muscular Dystrophy',\n",
       "   'Benign Brain Tumor',\n",
       "   'Encephalitis',\n",
       "   'Angioplasty and other Invasive Treatments for Coronary Artery Disease',\n",
       "   'Brain Surgery',\n",
       "   'Bacterial Meningitis',\n",
       "   'Serious Coronary Artery Disease',\n",
       "   'Loss of Independent Existence',\n",
       "   'Systemic Lupus Erythematosus with Severe Kidney Complications',\n",
       "   'Full-blown AIDS',\n",
       "   'Medullary Cystic Disease',\n",
       "   'Occupationally Acquired HIV',\n",
       "   'Terminal Illness',\n",
       "   'Apallic Syndrome',\n",
       "   'Poliomyelitis',\n",
       "   'Progressive Scleroderma',\n",
       "   'Chronic Relapsing Pancreatitis',\n",
       "   'Elephantiasis',\n",
       "   'Creutzfeldt-Jakob Disease']},\n",
       " {'_id': ObjectId('67171ded2ca774a8bfd83352'),\n",
       "  'demographic': 'general',\n",
       "  'description': 'Assured Members (employees/members) and their legal spouse aged between nineteen (19) to sixty (60) years next birthday.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the mongo documents based on their collection and ids\n",
    "from bson.objectid import ObjectId as oid \n",
    "def find_documents(collection_matches : list, verbose=False, database=db) -> list:\n",
    "    \"\"\"use the collection matches to find the corresponding documents in the specified mongo database.\n",
    "\n",
    "    Args:\n",
    "        collection_matches (list): list of document metadata containing collection names and ids\n",
    "        verbose (bool, optional): flag to turn on debug mode. Defaults to False.\n",
    "        database (pymongo.synchronous.database.Database, optional): the mongodb database to use. Defaults to the db variable.\n",
    "\n",
    "    Returns:\n",
    "        list: list of mongodb documents\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Finding documents...\")\n",
    "    documents = []\n",
    "    for collection_match in collection_matches:\n",
    "        collection = database[collection_match['collection']]\n",
    "        if 'ids' in collection_match:\n",
    "            for id in collection_match['ids']:\n",
    "                documents.append(collection.find_one({\"_id\": oid(id)}))\n",
    "        else:\n",
    "            documents.append(collection.find_one({\"_id\": oid(collection_match['id'])}))\n",
    "    return documents\n",
    "\n",
    "documents = find_documents(document_metadata)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the previous 3 functions only allow access to unstructured data, but ignores structured data like tables   \n",
    "add a component for fetching structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#setup the chatbot model for tool use\n",
    "from langchain_core.tools import tool\n",
    "import re\n",
    "\n",
    "@tool\n",
    "def fetch_metrics(question: str) -> list:\n",
    "    \"\"\"Retrieves only JSON of table data for either premium plans, total investment estimations, premium allocation or fund performance based on the user query.\"\"\"\n",
    "    cleaned_question = preprocess_text(question)\n",
    "    documents = []\n",
    "    has_fetched_preimum_plans = False\n",
    "    has_fetched_total_investment_estimation = False\n",
    "    has_fetched_fund = False\n",
    "    has_fetched_premium_allocation = False\n",
    "    for keyword in re.findall(r'\\b\\w+\\b', cleaned_question):\n",
    "        match keyword:\n",
    "            case \"premium\" | \"plan\" | \"coverage\" | \"price\" | \"pricing\":\n",
    "                if not has_fetched_preimum_plans:\n",
    "                    print(\"fetching premium plans\")\n",
    "                    \n",
    "                    documents.append(list(db[\"premium_plans\"].find()))\n",
    "                    has_fetched_preimum_plans = True\n",
    "            case \"investment\" | \"value\" | \"allocation\":\n",
    "                if not has_fetched_total_investment_estimation:\n",
    "                    print(\"fetching investment plans\")\n",
    "                    \n",
    "                    documents.append(list(db[\"total_investment_estimations\"].find()))\n",
    "                    has_fetched_total_investment_estimation = True\n",
    "            case \"performance\" | \"perform\" | \"fund\":\n",
    "                if not has_fetched_fund:\n",
    "                    print(\"fetching fund performance\")\n",
    "                    \n",
    "                    documents.append(list(db[\"funds\"].find()))\n",
    "                    has_fetched_fund = True\n",
    "            case \"allocation\":\n",
    "                if not has_fetched_premium_allocation:\n",
    "                    print(\"fetching premium allocation\")\n",
    "                    \n",
    "                    documents.append(list(db[\"premium_allocations\"].find()))\n",
    "                    has_fetched_premium_allocation = True\n",
    "            case _:\n",
    "                continue\n",
    "\n",
    "    return documents\n",
    "print()\n",
    "@tool\n",
    "def get_context(question: str, debug: bool = False) -> list:\n",
    "    \"\"\"Retrieves text-based information for an insurance product only based on the user query.\n",
    "does not answer quwstions about the chat agent\"\"\"\n",
    "    print(question)\n",
    "    matches = semantic_search(question, verbose=debug)\n",
    "    document_data = get_collection_matches(matches, verbose=debug)\n",
    "    context = find_documents(document_data, verbose=debug)\n",
    "    return context\n",
    "\n",
    "tools = [fetch_metrics,\n",
    "        get_context\n",
    "        ]\n",
    "toolPicker = Ollama(model='llama3.2').bind_tools(tools)\n",
    "# for group in fetch_metrics(\"i need information on performing funds, how much my premium is allocated, the allocation rate, the pricing plan, how much will i be paying per premium payment, what is the coverage performance, what are the fundings\"):\n",
    "#     for document in group:\n",
    "#         print(document)\n",
    "\n",
    "# fetch_metrics(\"i need information on performing funds. how much my premium is allocated? the allocation rate, the pricing plan, how much will i be paying per premium payment, what is the coverage performance, what are the fundings, testing the word post-apocalyptic, 80 90% 20$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the RAG workflow\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser as parser\n",
    "# from langchain.schema.runnable import RunnablePassthrough\n",
    "from pydantic import ValidationError\n",
    "\n",
    "def RAG(query : str, verbose : bool | None = False):\n",
    "    if query in [\"exit\", \"quit\", \"bye\", \"end\", \"stop\", \"\"]:\n",
    "        return\n",
    "    if verbose:\n",
    "        tool_query=query+\" (Turn on debug mode)\"\n",
    "    else:\n",
    "        tool_query=query\n",
    "    messages = [query]\n",
    "    \n",
    "    ai_msg = toolPicker.invoke(tool_query)\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        selected_tool = {\"fetch_metrics\": fetch_metrics,\n",
    "                        \"get_context\": get_context\n",
    "                        }[tool_call[\"name\"].lower()]\n",
    "\n",
    "        tool_msg = None \n",
    "        try:\n",
    "            tool_msg = selected_tool.invoke(tool_call)\n",
    "        except ValidationError as e:\n",
    "            print(f\"Error: {e.json()}\")\n",
    "        print(f\"called {tool_call[\"name\"].lower()} with {tool_call['args']}\")\n",
    "        messages.append(f\"connected {tool_call[\"name\"].lower()} function returned {tool_msg.content}\")\n",
    "\n",
    "    if verbose:\n",
    "        for message in messages:\n",
    "            print(message)\n",
    "\n",
    "    \n",
    "    return chatbot_model.stream(f\"\"\"fullfill the query with the provided information\n",
    "query      :{query}\n",
    "information:{messages}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup a simple intent classifier\n",
    "def classify_intent(user_input:str) -> str:\n",
    "    \"\"\"classify the intent of the user input\n",
    "\n",
    "current implementation uses a lightweight model (llama3.2:1B)\n",
    "with few-shot prompting examples\n",
    "for classifying 'normal', 'register', 'RAG' intents\n",
    "\n",
    "\"\"\"\n",
    "    return intent_classifier.invoke(f\"\"\"Classify the given input, answer only with 'normal','register','RAG':\n",
    "\n",
    "example:\n",
    "\n",
    "Input: \"Is there a contact number\", Intent: RAG\n",
    "Input: \"How do I create a new account\", Intent: register\n",
    "Input: \"How do I make a claim\", Intent: RAG\n",
    "Input: \"Search the web for cat videos\", Intent: normal\n",
    "Input: \"Help me register for this service\", Intent: register\n",
    "Input: \"Where can I get started\", Intent: register\n",
    "Input: \"What is your name\", Intent: normal\n",
    "Input: \"What entities are attached to this service\", Intent: RAG\n",
    "Input: \"What is your purpose\", Intent: normal\n",
    "Input: \"Can I see some fund performance metrics\", Intent: RAG\n",
    "Input: \"What is the weather in your country\", Intent: normal\n",
    "Input: \"How can i register for an account\", Intent: register\n",
    "Input: \"Who owns the product\", Intent: RAG\n",
    "Input: \"Tell me how to subscribe\", Intent: register\n",
    "Input: \"Guide me through the registration process\", Intent: register\n",
    "Input: \"How do I sign up for the trial\", Intent: register\n",
    "Input: \"Can you explain your features\", Intent: normal\n",
    "Input: \"Sign me up\", Intent: register\n",
    "Input: \"Can you assist me with enrolling\", Intent: register\n",
    "Input: \"What services does the product provide\", Intent: RAG\n",
    "Input: \"Where can i wash my dog\", Intent: normal\n",
    "Input: \"Goodbye\", Intent: normal\n",
    "Input: \"How do i pay for the service\", Intent: RAG\n",
    "Input: \"how is my premium allocated\", Intent: RAG\n",
    "Input: \"Hello\", Intent: normal\n",
    "Input: \"What are the coverage options\", Intent: RAG\n",
    "Input: \"What's the first step to register\", Intent: register\n",
    "Input: \"What funds are involved\", Intent: RAG\n",
    "Input: \"Where are you located\", Intent: normal\n",
    "Input: \"Who do I contact for help\", Intent: RAG\n",
    "Input: \"Can I pay with a credit card\", Intent: RAG\n",
    "Input: \"Why should i trust you\", Intent: normal\n",
    "Input: \"What should i get ready for enrollment\", Intent: register\n",
    "Input: \"How are you\", Intent\": normal\n",
    "Input: \"What company distributes this service\", Intent: RAG\n",
    "Input: \"Are there any additional charges\", Intent: RAG\n",
    "\n",
    "Input: {user_input}\"\"\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input: str) -> None:\n",
    "    intent = classify_intent(user_input)\n",
    "    if intent == \"RAG\":\n",
    "        stream = RAG(user_input)\n",
    "    elif intent == \"register\":\n",
    "        print(\"register function work in progress\")\n",
    "    else:\n",
    "        stream = chatbot_model.stream(user_input)\n",
    "    for chunk in stream:\n",
    "        print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covered medical conditions\n",
      "{'read_units': 5}\n",
      "called get_context with {'debug': False, 'question': 'covered medical conditions'}\n",
      "Based on the provided information, the covered medical conditions are:\n",
      "\n",
      "1. Critical Illnesses:\n",
      "   - Heart Attack - of specified severity\n",
      "   - Stroke - resulting in permanent neurological deficit\n",
      "   - Coronary Artery By-pass Surgery\n",
      "   - Cancer - of specified severity\n",
      "   - Kidney Failure - requiring dialysis or transplant\n",
      "   - Fulminant Viral Hepatitis\n",
      "   - Major Organ/Bone Marrow Transplant\n",
      "   - Paralysis of Limbs\n",
      "   - Multiple Sclerosis\n",
      "   - Primary Pulmonary Arterial Hypertension\n",
      "   - Blindness - Permanent and Irreversible\n",
      "   - Heart Valve Surgery\n",
      "   - Deafness - Permanent and Irreversible\n",
      "   - Surgery to Aorta\n",
      "   - Loss of Speech\n",
      "   - Alzheimer's Disease / Severe Dementia\n",
      "   - Third Degree Burns - of specified severity\n",
      "   - Coma - resulting in permanent neurological deficit\n",
      "   - Cardiomyopathy - of specified severity\n",
      "   - Motor Neuron Disease\n",
      "   - HIV Infection due to Blood Transfusion\n",
      "   - Parkinson's Disease\n",
      "   - End-Stage Liver Failure\n",
      "   - End-Stage Lung Disease\n",
      "   - Major Head Trauma\n",
      "   - Chronic Aplastic Anemia\n",
      "   - Muscular Dystrophy\n",
      "   - Benign Brain Tumor\n",
      "   - Encephalitis\n",
      "   - Angioplasty and other Invasive Treatments for Coronary Artery Disease\n",
      "   - Brain Surgery\n",
      "   - Bacterial Meningitis\n",
      "   - Serious Coronary Artery Disease\n",
      "   - Loss of Independent Existence\n",
      "   - Systemic Lupus Erythematosus with Severe Kidney Complications\n",
      "   - Full-blown AIDS\n",
      "   - Medullary Cystic Disease\n",
      "   - Occupationally Acquired HIV\n",
      "   - Terminal Illness\n",
      "   - Apallic Syndrome\n",
      "   - Poliomyelitis\n",
      "   - Progressive Scleroderma\n",
      "   - Chronic Relapsing Pancreatitis\n",
      "   - Elephantiasis\n",
      "   - Creutzfeldt-Jakob Disease\n",
      "\n",
      "2. Critical Illness and TPD (Total Permanent Disability):\n",
      "   - Any Critical Illness or TPD that meets the defined criteria.\n",
      "\n",
      "3. Hospitalisation Income Benefit:\n",
      "   - Any illness or injury that requires hospitalization for more than 30 days.\n",
      "   - Any injury or accident during the period of insurance.\n",
      "\n",
      "4. Death due to Natural Causes:\n",
      "   - Death due to illness or natural cause, with no pre-existing condition affecting coverage.\n",
      "\n",
      "Please note that some conditions may have specific requirements or exclusions, and it's always best to consult with a medical professional or the insurance provider for detailed information on covered conditions."
     ]
    }
   ],
   "source": [
    "chat(\"what are the covered medical conditions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
