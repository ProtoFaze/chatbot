{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# %pip install -U python-dotenv langchain langchain_community langchain_mongodb nltk langchain-ollama langchain-pinecone pinecone-notebooks einops langgraph langchain_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assuming you have a JSON copy of the data   \n",
    "Data aquisition:   \n",
    "import the json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# load a json file as data\n",
    "data  = json.loads(Path(\"../data/data.json\").read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data ingestion:   \n",
    "since it is better to supply the full context of structured and unstructured data fetched in the future,   \n",
    "we will store the raw text in mongodb first   \n",
    "and apply preprocessing later when we need semantically/meaning accurate search results (important for vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to your MongoDB Atlas(Cloud) cluster\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient(os.environ[\"MONGODB_URI\"])\n",
    "db = client[os.environ[\"MONGODB_DB\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'currency': 'RM', 'premium_per_month': 30, 'coverage': {'critical_illnesses': 20000, 'death_natural': 20000, 'accidental_death': 40000, 'TPD_illness': 20000, 'TPD_accident': 40000, 'hospitalisation_income_per_day': 20, 'funeral_expenses': 5000}}, {'currency': 'RM', 'premium_per_month': 45, 'coverage': {'critical_illnesses': 30000, 'death_natural': 30000, 'accidental_death': 60000, 'TPD_illness': 30000, 'TPD_accident': 60000, 'hospitalisation_income_per_day': 30, 'funeral_expenses': 5000}}]\n",
      "{'disclaimer': 'Upon payment of claim for Angioplasty and other invasive treatments for coronary artery disease, the sum assured will be reduced by the quantum of the payment for Angioplasty and other invasive treatments for coronary artery disease. However, the premium shall remain unchanged. This benefit is subject to a limit of RM1,000,000 under the Policy and all other non-credit-related group policies (including supplementary contract and endorsement, if any) issued by the Company by any name or description which provide Critical Illness benefits or similar benefits on the same Life Assured. Complete definition of Critical Illness as mentioned in the Master Policy need to be fulfilled before any 45 Critical Illnesses claim can become payable.', 'illnesses': ['Heart Attack - of specified severity', 'Stroke - resulting in permanent neurological deficit', 'Coronary Artery By-pass Surgery', 'Cancer - of specified severity', 'Kidney Failure - requiring dialysis or transplant', 'Fulminant Viral Hepatitis', 'Major Organ/Bone Marrow Transplant', 'Paralysis of Limbs', 'Multiple Sclerosis', 'Primary Pulmonary Arterial Hypertension', 'Blindness - Permanent and Irreversible', 'Heart Valve Surgery', 'Deafness - Permanent and Irreversible', 'Surgery to Aorta', 'Loss of Speech', \"Alzheimer's Disease / Severe Dementia\", 'Third Degree Burns - of specified severity', 'Coma - resulting in permanent neurological deficit', 'Cardiomyopathy - of specified severity', 'Motor Neuron Disease', 'HIV Infection due to Blood Transfusion', \"Parkinson's Disease\", 'End-Stage Liver Failure', 'End-Stage Lung Disease', 'Major Head Trauma', 'Chronic Aplastic Anemia', 'Muscular Dystrophy', 'Benign Brain Tumor', 'Encephalitis', 'Angioplasty and other Invasive Treatments for Coronary Artery Disease', 'Brain Surgery', 'Bacterial Meningitis', 'Serious Coronary Artery Disease', 'Loss of Independent Existence', 'Systemic Lupus Erythematosus with Severe Kidney Complications', 'Full-blown AIDS', 'Medullary Cystic Disease', 'Occupationally Acquired HIV', 'Terminal Illness', 'Apallic Syndrome', 'Poliomyelitis', 'Progressive Scleroderma', 'Chronic Relapsing Pancreatitis', 'Elephantiasis', 'Creutzfeldt-Jakob Disease']}\n",
      "[{'condition': 'Death', 'sum_assured_percentage': 100, 'section': 'A'}, {'condition': 'Loss of both hands', 'sum_assured_percentage': 100, 'section': 'B'}, {'condition': 'Loss of both feet', 'sum_assured_percentage': 100, 'section': 'B'}, {'condition': 'Complete and irrecoverable loss of sight in both eyes', 'sum_assured_percentage': 100, 'section': 'B'}, {'condition': 'Loss of one hand and one foot', 'sum_assured_percentage': 100, 'section': 'B'}, {'condition': 'Injuries resulting in permanently being bedridden', 'sum_assured_percentage': 100, 'section': 'B'}, {'condition': 'Any other injuries resulting in permanent total disablement', 'sum_assured_percentage': 100, 'section': 'B'}, {'condition': 'Complete and incurable paralysis', 'sum_assured_percentage': 100, 'section': 'B'}]\n",
      "[{'years': 3, 'insured_age_next_birthday': 65, 'contribution_per_month': 50, 'Total_Contribution': 1800, 'Projected_Return_Scenario_X_2_percent': 490, 'Projected_Return_Scenario_Y_5_percent': 511}, {'years': 5, 'insured_age_next_birthday': 65, 'contribution_per_month': 50, 'Total_Contribution': 3000, 'Projected_Return_Scenario_X_2_percent': 819, 'Projected_Return_Scenario_Y_5_percent': 879}, {'years': 10, 'insured_age_next_birthday': 65, 'contribution_per_month': 50, 'Total_Contribution': 6000, 'Projected_Return_Scenario_X_2_percent': 1653, 'Projected_Return_Scenario_Y_5_percent': 1902}, {'years': 15, 'insured_age_next_birthday': 65, 'contribution_per_month': 50, 'Total_Contribution': 9000, 'Projected_Return_Scenario_X_2_percent': 2500, 'Projected_Return_Scenario_Y_5_percent': 3095}, {'years': 20, 'insured_age_next_birthday': 65, 'contribution_per_month': 50, 'Total_Contribution': 12000, 'Projected_Return_Scenario_X_2_percent': 3652, 'Projected_Return_Scenario_Y_5_percent': 4790}]\n",
      "[{'policy_year_range_start': 1, 'policy_year_range_end': 16, 'contribution_per_month': 50, 'total_premium_paid': 600, 'percentage': 88, 'amount': 528}, {'policy_year_range_start': 17, 'policy_year_range_end': None, 'contribution_per_month': 50, 'total_premium_paid': 600, 'percentage': 100, 'amount': 600}]\n",
      "{'type': 'child', 'description': 'if the death, Total and Permanent Disability (TPD) or diagnosis of Covered Event was made before the Assured Child attained age five (5) years next birthday, the benefits for Death, Total & Permanent Disability and Covered Event shall be reduced in accordance with the following table', 'claim_percentage_of': 'basic sum assured', 'claim': [{'age_next_birthday': 1, 'claim_percentage': 20}, {'age_next_birthday': 2, 'claim_percentage': 40}, {'age_next_birthday': 3, 'claim_percentage': 60}, {'age_next_birthday': 4, 'claim_percentage': 80}, {'age_next_birthday': 5, 'claim_percentage': 100}]}\n",
      "{'name': 'Dana Gemilang', 'description': ['The Dana Gemilang is a fund where 80% to 100% of the investments are in equities, which may be volatile in the short term.', \"This fund seeks to achieve medium to long-term capital appreciation. Although the fund invests mainly in Malaysia (50% to 100%), it may also partially invest in Singapore (up to 25%) and Hong Kong (up to 25%), if and when necessary, to enhance the fund's returns.\", 'The fund only invests in Shariah-approved securities. Although this fund invests in Shariah-approved securities, the investment-linked insurance plan that utilises this fund is not a Shariah-compliant product.'], 'fund_performance': [{'year': 2017, 'return_percentage': 13.95}, {'year': 2018, 'return_percentage': -17.84}, {'year': 2019, 'return_percentage': 4.14}, {'year': 2020, 'return_percentage': 1.77}, {'year': 2021, 'return_percentage': -2.52}]}\n"
     ]
    }
   ],
   "source": [
    "tables = [\n",
    "    {\"name\":\"premium_plans\", \"field_containing_data\":\"all\"},\n",
    "    {\"name\":\"critical_illnesses\", \"field_containing_data\":\"illnesses\"},\n",
    "    {\"name\":\"schedule_of_compensation\",\"field_containing_data\":\"all\"},\n",
    "    {\"name\":\"total_investment_estimations\", \"field_containing_data\":\"all\"},\n",
    "    {\"name\":\"premium_allocations\",\"field_containing_data\":\"all\"},\n",
    "    {\"name\":\"claims\",\"field_containing_data\":\"claim\"},\n",
    "    {\"name\":\"funds\",\"field_containing_data\":\"fund_performance\"},\n",
    "    ]\n",
    "for table in tables:\n",
    "    print(data[table['name']])\n",
    "    table['table'] = data[table['name']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama as Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser as parser\n",
    "\n",
    "# pipe = HuggingFacePipeline.from_model_id(model_id=\"meta-llama/Llama-3.2-3B\",task = \"text-generation\")\n",
    "\n",
    "def tables_summarize(tables):\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "                    Give a concise summary of the table or text as sentences. \\\n",
    "                    Do not include text like \"here is a summary of the table\" in your responses \\\n",
    "                    Table or text chunk: {element} \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "    model = Ollama(temperature=0, model=\"llama3.2\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | parser()\n",
    "    table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "    \n",
    "        \n",
    "    #text_summaries =  summarize_chain.batch(data_category[0], {\"max_concurrency\": 5})# no need to summarize\n",
    "\n",
    "    return table_summaries\n",
    "# table_summaries = tables_summarize(tables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'table': [{'currency': 'RM', 'premium_per_month': 30, 'coverage': {'critical_illnesses': 20000, 'death_natural': 20000, 'accidental_death': 40000, 'TPD_illness': 20000, 'TPD_accident': 40000, 'hospitalisation_income_per_day': 20, 'funeral_expenses': 5000}}, {'currency': 'RM', 'premium_per_month': 45, 'coverage': {'critical_illnesses': 30000, 'death_natural': 30000, 'accidental_death': 60000, 'TPD_illness': 30000, 'TPD_accident': 60000, 'hospitalisation_income_per_day': 30, 'funeral_expenses': 5000}}], 'summary': 'The table contains information on premium plans for a life insurance policy. The two plans have different monthly premiums and coverage amounts for various benefits such as critical illnesses, death due to natural causes, accidental deaths, and hospitalization income per day. The plans also include funeral expenses coverage.'}\n",
      "{'disclaimer': 'Upon payment of claim for Angioplasty and other invasive treatments for coronary artery disease, the sum assured will be reduced by the quantum of the payment for Angioplasty and other invasive treatments for coronary artery disease. However, the premium shall remain unchanged. This benefit is subject to a limit of RM1,000,000 under the Policy and all other non-credit-related group policies (including supplementary contract and endorsement, if any) issued by the Company by any name or description which provide Critical Illness benefits or similar benefits on the same Life Assured. Complete definition of Critical Illness as mentioned in the Master Policy need to be fulfilled before any 45 Critical Illnesses claim can become payable.', 'illnesses': ['Heart Attack - of specified severity', 'Stroke - resulting in permanent neurological deficit', 'Coronary Artery By-pass Surgery', 'Cancer - of specified severity', 'Kidney Failure - requiring dialysis or transplant', 'Fulminant Viral Hepatitis', 'Major Organ/Bone Marrow Transplant', 'Paralysis of Limbs', 'Multiple Sclerosis', 'Primary Pulmonary Arterial Hypertension', 'Blindness - Permanent and Irreversible', 'Heart Valve Surgery', 'Deafness - Permanent and Irreversible', 'Surgery to Aorta', 'Loss of Speech', \"Alzheimer's Disease / Severe Dementia\", 'Third Degree Burns - of specified severity', 'Coma - resulting in permanent neurological deficit', 'Cardiomyopathy - of specified severity', 'Motor Neuron Disease', 'HIV Infection due to Blood Transfusion', \"Parkinson's Disease\", 'End-Stage Liver Failure', 'End-Stage Lung Disease', 'Major Head Trauma', 'Chronic Aplastic Anemia', 'Muscular Dystrophy', 'Benign Brain Tumor', 'Encephalitis', 'Angioplasty and other Invasive Treatments for Coronary Artery Disease', 'Brain Surgery', 'Bacterial Meningitis', 'Serious Coronary Artery Disease', 'Loss of Independent Existence', 'Systemic Lupus Erythematosus with Severe Kidney Complications', 'Full-blown AIDS', 'Medullary Cystic Disease', 'Occupationally Acquired HIV', 'Terminal Illness', 'Apallic Syndrome', 'Poliomyelitis', 'Progressive Scleroderma', 'Chronic Relapsing Pancreatitis', 'Elephantiasis', 'Creutzfeldt-Jakob Disease'], 'summary': 'The policy has a critical illness benefit that covers 45 specified illnesses, including heart attack, stroke, cancer, kidney failure, and more. The sum assured will be reduced by the payment for these treatments if claimed. The premium remains unchanged despite this reduction. There is a limit of RM1,000,000 under the Policy for these benefits.'}\n",
      "{'table': [{'condition': 'Death', 'sum_assured_percentage': 100, 'section': 'A'}, {'condition': 'Loss of both hands', 'sum_assured_percentage': 100, 'section': 'B'}, {'condition': 'Loss of both feet', 'sum_assured_percentage': 100, 'section': 'B'}, {'condition': 'Complete and irrecoverable loss of sight in both eyes', 'sum_assured_percentage': 100, 'section': 'B'}, {'condition': 'Loss of one hand and one foot', 'sum_assured_percentage': 100, 'section': 'B'}, {'condition': 'Injuries resulting in permanently being bedridden', 'sum_assured_percentage': 100, 'section': 'B'}, {'condition': 'Any other injuries resulting in permanent total disablement', 'sum_assured_percentage': 100, 'section': 'B'}, {'condition': 'Complete and incurable paralysis', 'sum_assured_percentage': 100, 'section': 'B'}], 'summary': 'The schedule of compensation outlines specific conditions under which a certain percentage of the sum assured is paid. These conditions include death, loss of both hands or feet, complete and irrecoverable loss of sight in both eyes, permanent total disablement due to injury, and complete and incurable paralysis.'}\n",
      "{'table': [{'years': 3, 'insured_age_next_birthday': 65, 'contribution_per_month': 50, 'Total_Contribution': 1800, 'Projected_Return_Scenario_X_2_percent': 490, 'Projected_Return_Scenario_Y_5_percent': 511}, {'years': 5, 'insured_age_next_birthday': 65, 'contribution_per_month': 50, 'Total_Contribution': 3000, 'Projected_Return_Scenario_X_2_percent': 819, 'Projected_Return_Scenario_Y_5_percent': 879}, {'years': 10, 'insured_age_next_birthday': 65, 'contribution_per_month': 50, 'Total_Contribution': 6000, 'Projected_Return_Scenario_X_2_percent': 1653, 'Projected_Return_Scenario_Y_5_percent': 1902}, {'years': 15, 'insured_age_next_birthday': 65, 'contribution_per_month': 50, 'Total_Contribution': 9000, 'Projected_Return_Scenario_X_2_percent': 2500, 'Projected_Return_Scenario_Y_5_percent': 3095}, {'years': 20, 'insured_age_next_birthday': 65, 'contribution_per_month': 50, 'Total_Contribution': 12000, 'Projected_Return_Scenario_X_2_percent': 3652, 'Projected_Return_Scenario_Y_5_percent': 4790}], 'summary': 'The table contains data on total investment estimations for different scenarios. The scenarios include varying years of contribution and projected returns based on two different scenarios (X_2 percent and Y_5 percent). The contributions range from $1800 to $12000 over a period of 3 to 20 years, with the insured age next birthday remaining constant at 65.'}\n",
      "{'table': [{'policy_year_range_start': 1, 'policy_year_range_end': 16, 'contribution_per_month': 50, 'total_premium_paid': 600, 'percentage': 88, 'amount': 528}, {'policy_year_range_start': 17, 'policy_year_range_end': None, 'contribution_per_month': 50, 'total_premium_paid': 600, 'percentage': 100, 'amount': 600}], 'summary': 'The table contains two policy allocations. The first allocation spans from policy year range start 1 to end 16, with a monthly contribution of $50 and total premium paid of $600, representing 88% of the amount ($528). The second allocation starts at policy year range start 17 but ends at None, with the same monthly contribution and total premium paid as the first allocation, but represents 100% of the amount ($600).'}\n",
      "{'type': 'child', 'description': 'if the death, Total and Permanent Disability (TPD) or diagnosis of Covered Event was made before the Assured Child attained age five (5) years next birthday, the benefits for Death, Total & Permanent Disability and Covered Event shall be reduced in accordance with the following table', 'claim_percentage_of': 'basic sum assured', 'claim': [{'age_next_birthday': 1, 'claim_percentage': 20}, {'age_next_birthday': 2, 'claim_percentage': 40}, {'age_next_birthday': 3, 'claim_percentage': 60}, {'age_next_birthday': 4, 'claim_percentage': 80}, {'age_next_birthday': 5, 'claim_percentage': 100}], 'summary': \"The benefits for Death, Total & Permanent Disability, and Covered Event are reduced if the Assured Child's death or diagnosis occurs before their fifth birthday. The reduction is based on a percentage of the basic sum assured, increasing from 20% at age one to 100% at age five.\"}\n",
      "{'name': 'Dana Gemilang', 'description': ['The Dana Gemilang is a fund where 80% to 100% of the investments are in equities, which may be volatile in the short term.', \"This fund seeks to achieve medium to long-term capital appreciation. Although the fund invests mainly in Malaysia (50% to 100%), it may also partially invest in Singapore (up to 25%) and Hong Kong (up to 25%), if and when necessary, to enhance the fund's returns.\", 'The fund only invests in Shariah-approved securities. Although this fund invests in Shariah-approved securities, the investment-linked insurance plan that utilises this fund is not a Shariah-compliant product.'], 'fund_performance': [{'year': 2017, 'return_percentage': 13.95}, {'year': 2018, 'return_percentage': -17.84}, {'year': 2019, 'return_percentage': 4.14}, {'year': 2020, 'return_percentage': 1.77}, {'year': 2021, 'return_percentage': -2.52}], 'summary': \"The Dana Gemilang fund invests 80-100% in equities and aims for medium to long-term capital appreciation. The fund primarily invests in Malaysia (50-100%) but may also invest in Singapore and Hong Kong if necessary. It only investes in Shariah-approved securities. The fund's performance has been volatile, with returns ranging from -17.84% in 2018 to -2.52% in 2021.\"}\n"
     ]
    }
   ],
   "source": [
    "summaries = {}\n",
    "for index, value in enumerate(table_summaries):\n",
    "    summaries[tables[index][\"name\"]] = value\n",
    "\n",
    "for key, value in summaries.items():\n",
    "    if isinstance(data[key],list) & (len(data[key])>1):\n",
    "        collection = {\"table\":data[key]}\n",
    "    else:\n",
    "        collection = data[key]\n",
    "    collection[\"summary\"] = value\n",
    "    data[key] = collection\n",
    "    print(data[key])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_targets already exists in the database, skipping insertion\n",
      "insertion complete\n"
     ]
    }
   ],
   "source": [
    "#specify outer level json objects as mongodb collection names, then based on the specified names, create new collections and insert corresponding data\n",
    "#specify all the collections\n",
    "collection_names = []\n",
    "for key, value in data.items():\n",
    "    collection_names.append(key)\n",
    "    # if type(value) != str:\n",
    "    #     value = json.dumps(value)\n",
    "    # print(key+\":    \"+value)\n",
    "    \n",
    "#fetch existing collections\n",
    "ignored_and_existing_collections = db.list_collection_names()\n",
    "\n",
    "#specify the collections to ignore\n",
    "if \"questions_and_answers2\" not in ignored_and_existing_collections:\n",
    "        ignored_and_existing_collections.append(\"questions_and_answers2\")\n",
    "if \"exclusions2\" not in ignored_and_existing_collections:\n",
    "        ignored_and_existing_collections.append(\"exclusions2\")\n",
    "\n",
    "for collection_name in collection_names:\n",
    "    if collection_name in ignored_and_existing_collections:\n",
    "       continue\n",
    "    collection = db[collection_name]\n",
    "    if isinstance(data[collection_name], list):\n",
    "        print(f\"\"\"\n",
    "inserting many records of {data[collection_name]} into {collection_name} collection\n",
    "\"\"\")\n",
    "        collection.insert_many(data[collection_name])\n",
    "    else:\n",
    "        print(f\"\"\"\n",
    "inserting one record of {data[collection_name]} into {collection_name} collection\n",
    "\"\"\")\n",
    "        collection.insert_one(data[collection_name])\n",
    "\n",
    "# separately insert the vector targets\n",
    "collection = db[\"vector_targets\"]\n",
    "vector_target_records = list(collection.find())\n",
    "if len(vector_target_records) == 0:\n",
    "        print(f\"\"\"\n",
    "inserting one record of {data[\"vector_targets\"]} into {collection_name} collection\n",
    "\"\"\")\n",
    "        collection.insert_many(data[\"vector_targets\"])        \n",
    "else:\n",
    "    print(\"vector_targets already exists in the database, skipping insertion\")\n",
    "\n",
    "print(\"insertion complete\")\n",
    "\n",
    "ignored_and_existing_collections = None\n",
    "del ignored_and_existing_collections\n",
    "collection_names = None\n",
    "del collection_names\n",
    "vector_target_records = None\n",
    "del vector_target_records\n",
    "data = None\n",
    "del data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further preprocessing:   \n",
    "lowercasing   \n",
    "special character removal\n",
    "\n",
    "stop words removal\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use nltk to lowercase, remove special characters, tokenize, remove stopwords, and lemmatize the values in the json data\n",
    "#fetch all data cleaning resources\n",
    "import nltk\n",
    "import re\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet.zip')\n",
    "except:\n",
    "    print(\"sam ting wong\")\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#initialize the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#allow specific stopwords\n",
    "stop_words.remove(\"both\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    pattern = re.compile(r\"[;@#&*!()\\[\\]]\")\n",
    "    def get_wordnet_pos(tag):\n",
    "        match tag[0]:\n",
    "            case 'J':\n",
    "                return wordnet.ADJ\n",
    "            case 'V':\n",
    "                return wordnet.VERB\n",
    "            case 'R':\n",
    "                return wordnet.ADV\n",
    "            case _:\n",
    "                return wordnet.NOUN\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # Remove stop words and lemmatize\n",
    "    processed_tokens = [\n",
    "        lemmatizer.lemmatize(word.lower(), get_wordnet_pos(tag)) \n",
    "        for word, tag in pos_tags \n",
    "        if word.lower() not in stop_words and not pattern.match(word)\n",
    "    ]\n",
    "    # Join tokens back to a string\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "#setup a function to recursively preprocess the json data\n",
    "def traverse_json(data, json_text_action, target_key=None):\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, str) and (target_key is None or key == target_key):\n",
    "                # Preprocess the string value\n",
    "                data[key] = json_text_action(value)\n",
    "            elif isinstance(value, dict) or isinstance(value, list):\n",
    "                # Recursively preprocess the dictionary or list\n",
    "                traverse_json(value, json_text_action)\n",
    "    elif isinstance(data, list):\n",
    "        for i, element in enumerate(data):\n",
    "            if isinstance(element, str):\n",
    "                # Preprocess the string in the list and save it in the original position\n",
    "                data[i] = json_text_action(element)\n",
    "            elif isinstance(element, dict) or isinstance(element, list):\n",
    "                # Recursively preprocess the dictionary or list\n",
    "                traverse_json(element, json_text_action)\n",
    "    return data\n",
    "\n",
    "# preprocess_text(\"i need information on performing funds. how much my premium is allocated? the allocation rate, the pricing plan, how much will i be paying per premium payment, what is the coverage performance, what are the fundings, testing the word post-apocalyptic, 80 90% 20$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/KINGSTON SNV2S1000G Media/prototypes/python/chatbot/.venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Connect to your pinecone vector database and specify the working index\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(os.environ[\"PINECONE_API_KEY\"])\n",
    "index = None\n",
    "if os.environ[\"PINECONE_INDEX_NAME\"] not in [index.name for index in pc.list_indexes()]:\n",
    "    index = pc.create_index(name = os.environ[\"PINECONE_INDEX_NAME\"],\n",
    "                            dimension = 768, \n",
    "                            metric = \"cosine\",\n",
    "                            spec = ServerlessSpec(cloud = \"aws\", region = \"us-east-1\") \n",
    "                            )\n",
    "else:\n",
    "    index = pc.Index(os.environ[\"PINECONE_INDEX_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 51}},\n",
       " 'total_vector_count': 51}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data embedding//vectorization\n",
    "\n",
    "target descriptive values and generate embeddings for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model (https://huggingface.co/nomic-ai/nomic-embed-text-v1.5\")\n",
    "# %pip install sentence-transformers scipy\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1.5\", model_kwargs={\"trust_remote_code\":True})\n",
    "# global_embedding_model = SentenceTransformer(global_embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True))\n",
    "global_embedding_model = OllamaEmbeddings(model = 'nomic-embed-text')\n",
    "\n",
    "# number_of_dimensions = 768 #nomic-embed-text-v1.5 has 768 dimensions\n",
    "vector_store = PineconeVectorStore(index=index, embedding=global_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the target fields to embed\n",
    "from copy import deepcopy\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def vectorize(texts):\n",
    "    \"vectorize the provided texts using a globally declared model.\"    \n",
    "    vectors = global_embedding_model.encode(texts)\n",
    "    return vectors\n",
    "\n",
    "ignore_collections = [\"vector_targets\", \"premium_plans\",\"important_notice\",\"total_investment_estimations\",\"premium_allocations\",\"test2\", \"test\",\"questions_and_answers2\",\"exclusions2\"]\n",
    "\n",
    "def vectorize_and_index(mongo_database=db, vector_index=index, ignore_collections=ignore_collections, debug=False):\n",
    "    \"\"\"vectorize the target fields and store the embeddings in the pinecone index.\n",
    "    \n",
    "current implmentation does not support converting mongo collection name to pinecone namespaces.\"\"\"\n",
    "    if mongo_database == None:\n",
    "        raise ValueError(\"mongo_database is required, the mongo database should also have a vector_targets collection specifying the target fields to vectorize\")\n",
    "    if not vector_index:\n",
    "        raise ValueError(\"vector_index is required\")\n",
    "    documents = []\n",
    "    for collection_name in mongo_database.list_collection_names():\n",
    "        if collection_name in ignore_collections:\n",
    "            continue                                            #skip collections that should not to be vectorized\n",
    "        if debug:\n",
    "            print(f\"\\nIn {collection_name}\")\n",
    "        results = list(mongo_database[collection_name].find())\n",
    "        targets = list(mongo_database.vector_targets.find({\"name\": collection_name}))\n",
    "        for record in results:\n",
    "            if targets:\n",
    "                for target in targets:                              #iterate through the target fields from the same collection\n",
    "                    old_data = deepcopy(record[target['field']])              #store original info\n",
    "                    traverse_json(record, preprocess_text, target['field'])\n",
    "                    cleaned_data = record[target['field']]                     #clean the data\n",
    "                    # vector = vectorize(record[target['field']])                #store the vectorized embeddings\n",
    "                    if debug:\n",
    "                        print(f\"original: {old_data}\\ncleaned : {cleaned_data}\")\n",
    "                    if isinstance(cleaned_data, list):#check if the vectorized embeddings are of the correct length\n",
    "                        #if the vectorized embeddings length not correct, possibility of other datatypes, iterate through the iterable\n",
    "                        for j , sub_vector in enumerate(cleaned_data):\n",
    "                            if debug:\n",
    "                                print(f\"vectorizing item {j+1} : {cleaned_data[j]}\")\n",
    "                            documents.append(Document(\n",
    "                                            page_content = cleaned_data[j]\n",
    "                                            ,metadata = {\"original\":old_data[j]}\n",
    "                                            ,id = str(record['_id'])+\"-\"+collection_name+\"-\"+target['field']+\"-\"+str(j) \n",
    "                            ))\n",
    "                    else:   \n",
    "                        print(\"vectorizing string\")\n",
    "                        documents.append(Document(\n",
    "                                        page_content=cleaned_data\n",
    "                                        ,metadata={\"original\":old_data}\n",
    "                                        ,id = str(record['_id'])+\"-\"+collection_name+\"-\"+target['field'] \n",
    "                        ))\n",
    "                        pass                                        \n",
    "                        \n",
    "            else:\n",
    "                if debug:\n",
    "                    print(\"no matching target, skipping vectorization\")\n",
    "                continue\n",
    "            \n",
    "    vector_store.add_documents(documents=documents)\n",
    "\n",
    "# vectorize_and_index(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using local(ollama) base model as chatbot model\n",
    "intent_classifier = Ollama(model='llama3.2',#:1B\n",
    "                           temperature=0,\n",
    "                           num_predict=4,\n",
    "                           request_timeout=10,\n",
    "                           verbose=False )\n",
    "chatbot_model = Ollama(model='llama3.2', request_timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='67286e50987ce18efd0c3ee4-questions_and_answers-answer-2', metadata={'original': 'Fund Management Charge is 0.50% per annum. Note: The fees and charges levied may change from time to time.'}, page_content='fund management charge 0.50 % per annum . note : fee charge levy may change time time .'),\n",
       "  Document(id='67286e50987ce18efd0c3ee1-questions_and_answers-answer', metadata={'original': 'Yes. The Assured Member/spouse/children will be required to reapply by completing a standard Group Proposal form, subject to approval by the Company and up to the maximum benefit allowed'}, page_content='yes . assured member/spouse/children require reapply complete standard group proposal form , subject approval company maximum benefit allow'),\n",
       "  Document(id='67286e50987ce18efd0c3ec7-advantages-advantage', metadata={'original': 'Automatic premium remittance via credit card / bank deduction / JomPay ensures continuous protection.'}, page_content='automatic premium remittance via credit card / bank deduction / jompay ensure continuous protection .'),\n",
       "  Document(id='67286e50987ce18efd0c3ecc-requirements-description', metadata={'original': 'Assured Members (employees/members) and their legal spouse aged between nineteen (19) to sixty (60) years next birthday.'}, page_content='assured member employees/members legal spouse age nineteen 19 sixty 60 year next birthday .'),\n",
       "  Document(id='67286e50987ce18efd0c3ed3-benefits-description-1', metadata={'original': 'TPD in respect of any Life Assured must be certified by a registered medical practitioner appointed by the company, to have continued for at least six (6) months from the date of disability.'}, page_content='tpd respect life assured must certify registered medical practitioner appoint company , continue least six 6 month date disability .'),\n",
       "  Document(id='67286e50987ce18efd0c3ee2-questions_and_answers-answer', metadata={'original': 'The Company will pay full Death Benefit due to suicide occurring after twelve (12) months from the commencement of the assurance. The policy does not cover for suicide within twelve (12) months from the commencement of the assurance'}, page_content='company pay full death benefit due suicide occur twelve 12 month commencement assurance . policy cover suicide within twelve 12 month commencement assurance')]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup the semantic search function\n",
    "def semantic_search(question, vector_store=vector_store, index=index, top_k=6, verbose=False):\n",
    "    \"\"\"vectorizes the question and queries the pinecone index for the top 3 closest matches.\n",
    "\n",
    "current implementation does not support querying multiple namespaces or using mongoDB indexes.\n",
    "\n",
    "Args:\n",
    "    question (str): _description_\n",
    "    debug (bool, optional): _description_. Defaults to False.\n",
    "    embedding (SentenceTransformer, optional): _description_. Defaults to global_embedding_model.\n",
    "    index (Pinecone.Index, optional): _description_. Defaults to index.\n",
    "\n",
    "Returns:\n",
    "    list(QueryResponse): the top closest query results from the pinecone index\n",
    "    \"\"\"\n",
    "    if not index:\n",
    "        print(\"Index not found, please specify your pinecone or mongo search index\")\n",
    "        return\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Encoding question...\")\n",
    "        \n",
    "    matches = []\n",
    "    spaces = index.describe_index_stats()['namespaces']\n",
    "    for key, value in spaces.items():\n",
    "        res = vector_store.similarity_search(\n",
    "            query=question,\n",
    "            k=top_k\n",
    "        #     namespace=key,\n",
    "        )\n",
    "        matches.append(res)\n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "response = semantic_search(\"who manages this product\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting collection matches...\n",
      "no pinecone namespace found, checking id or additional metadata\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#setup a function to receive semantic search results and return the collection name and ids\n",
    "def get_collection_matches(response : list, verbose=False) -> list:\n",
    "    \"\"\"based on the response from a pinecone semantic search, extract and consolidate the collection name and ids of the matching documents.\n",
    "\n",
    "    Args:\n",
    "        response (list): a list of QueryResponse objects from a pinecone semantic search\n",
    "        verbose (bool, optional): flag to use debug mode. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: list of mongodb documents\n",
    "    \"\"\"\n",
    "    def extract_info(match: str) -> dict:\n",
    "        data_from_id = None\n",
    "        try:\n",
    "            data_from_id = match['id'].split(\"-\")\n",
    "        except TypeError as e: #probably a document object\n",
    "            data_from_id = match.id.split(\"-\")\n",
    "        match_id = data_from_id[0]\n",
    "        collection_name = data_from_id[1]\n",
    "        return {'collection':collection_name, 'id':match_id}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Getting collection matches...\")\n",
    "    document_metadata = []\n",
    "    for namespaces_or_documents in response:\n",
    "        is_using_default_namespace = False\n",
    "        if isinstance(namespaces_or_documents, list):\n",
    "            if verbose:\n",
    "                print(\"no pinecone namespace found, checking id or additional metadata\")\n",
    "            for metadata in namespaces_or_documents:\n",
    "                document = extract_info(metadata)\n",
    "                if document in document_metadata:\n",
    "                    if verbose:\n",
    "                        print(f\"Duplicate id {document['id']} for {document['collection']} found in fetch list, ignoring\")\n",
    "                    continue\n",
    "                document_metadata.append(document)\n",
    "        else: #probably a dictionary\n",
    "            document = extract_info(namespaces_or_documents)\n",
    "            if document in document_metadata:\n",
    "                if verbose:\n",
    "                    print(f\"Duplicate id {document['id']} for {document['collection']} found in fetch list, ignoring\")\n",
    "                continue\n",
    "            ids = []\n",
    "            for match in collection:\n",
    "                if verbose:\n",
    "                    print(match)\n",
    "                data_from_id = match['id'].split(\"-\")\n",
    "                match_id = data_from_id[0]\n",
    "                if is_using_default_namespace:\n",
    "                    collection = data_from_id[1]\n",
    "                #filter out duplicate ids\n",
    "                if match_id not in ids:\n",
    "                    ids.append(match_id)\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(f\"Duplicate id {match_id} for {collection} found in fetch list, ignoring\")\n",
    "                    continue\n",
    "                if is_using_default_namespace:\n",
    "                    document_metadata.append({'collection':collection, 'id':match_id})\n",
    "            document_metadata.append({'collection':collection, 'ids':ids})\n",
    "    return document_metadata\n",
    "\n",
    "document_metadata = get_collection_matches(response, verbose=True)\n",
    "# print(document_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('67286e50987ce18efd0c3ee4'),\n",
       "  'question': 'What are the current fees and charges?',\n",
       "  'answer': ['Insurance charges are Applicable to the sum assured, and vary depending on the average age profile and claim experience of the scheme.',\n",
       "   'Monthly Policy Fee is RM5.00',\n",
       "   'Fund Management Charge is 0.50% per annum. Note: The fees and charges levied may change from time to time.']},\n",
       " {'_id': ObjectId('67286e50987ce18efd0c3ee1'),\n",
       "  'question': 'Can the Assured Member/spouse/children apply to contribute more',\n",
       "  'answer': 'Yes. The Assured Member/spouse/children will be required to reapply by completing a standard Group Proposal form, subject to approval by the Company and up to the maximum benefit allowed'},\n",
       " {'_id': ObjectId('67286e50987ce18efd0c3ec7'),\n",
       "  'advantage': 'Automatic premium remittance via credit card / bank deduction / JomPay ensures continuous protection.'},\n",
       " {'_id': ObjectId('67286e50987ce18efd0c3ecc'),\n",
       "  'demographic': 'general',\n",
       "  'description': 'Assured Members (employees/members) and their legal spouse aged between nineteen (19) to sixty (60) years next birthday.'},\n",
       " {'_id': ObjectId('67286e50987ce18efd0c3ed3'),\n",
       "  'title': 'Critical Illness and TPD',\n",
       "  'description': ['Critical Illness Benefit will become payable once a registered medical practitioner has confirmed that the Life Assured is diagnosed with one of the 45 Critical Illnesses.',\n",
       "   'TPD in respect of any Life Assured must be certified by a registered medical practitioner appointed by the company, to have continued for at least six (6) months from the date of disability.',\n",
       "   'Coverage ceases once a claim is paid for either TPD Benefit or Critical Illness Benefit.'],\n",
       "  'proposals_accepted_under_special_promotion_with_no_underwriting': ['If a claim for TPD or Critical Illness arises within the first six (6) months from the commencement date of the insurance and such claims events are not due to accidents, 50% of the TPD Benefit or Critical Illness Benefit is payable, and the assurance terminates.',\n",
       "   'Full claim on TPD Benefit or Critical Illness Benefit is payable after six (6) months from the commencement date.',\n",
       "   'No benefit is payable for any TPD or Critical Illness arising from pre-existing TPD or Critical Illness or any Critical Illness diagnosed within the first sixty (60) days of the commencement date.'],\n",
       "  'proposal_accepted_with_underwriting': ['Full sum assured is payable upon occurrence of TPD or Critical Illness.',\n",
       "   'No benefit is payable for any Critical Illness existing before the effective coverage date or diagnosed within the first sixty (60) days.',\n",
       "   'No benefit is payable if TPD is due to pre-existing TPD.']},\n",
       " {'_id': ObjectId('67286e50987ce18efd0c3ee2'),\n",
       "  'question': 'Will the Company pay the full Death Benefit if the Assured Member commits suicide',\n",
       "  'answer': 'The Company will pay full Death Benefit due to suicide occurring after twelve (12) months from the commencement of the assurance. The policy does not cover for suicide within twelve (12) months from the commencement of the assurance'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the mongo documents based on their collection and ids\n",
    "from bson.objectid import ObjectId as oid \n",
    "def find_documents(collection_matches : list, verbose=False, database=db) -> list:\n",
    "    \"\"\"use the collection matches to find the corresponding documents in the specified mongo database.\n",
    "\n",
    "    Args:\n",
    "        collection_matches (list): list of document metadata containing collection names and ids\n",
    "        verbose (bool, optional): flag to turn on debug mode. Defaults to False.\n",
    "        database (pymongo.synchronous.database.Database, optional): the mongodb database to use. Defaults to the db variable.\n",
    "\n",
    "    Returns:\n",
    "        list: list of mongodb documents\n",
    "    \"\"\"\n",
    "    print(f\"Finding documents using {len(collection_matches)} references\") if verbose else None\n",
    "    documents = []\n",
    "    for collection_match in collection_matches:\n",
    "        collection = database[collection_match['collection']]\n",
    "        if 'ids' in collection_match:\n",
    "            for id in collection_match['ids']:\n",
    "                documents.append(collection.find_one({\"_id\": oid(id)}))\n",
    "        else:\n",
    "            documents.append(collection.find_one({\"_id\": oid(collection_match['id'])}))\n",
    "    return documents\n",
    "\n",
    "documents = find_documents(document_metadata)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the previous 3 functions only allow access to unstructured data, but ignores structured data like tables   \n",
    "add a component for fetching structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the chatbot model for tool use\n",
    "from langchain_core.tools import tool\n",
    "import re\n",
    "\n",
    "@tool\n",
    "def fetch_metrics(question: str) -> list:\n",
    "    \"\"\"Retrieves only JSON of table data for either premium plans, total investment estimations, premium allocation or fund performance based on the user query.\"\"\"\n",
    "    cleaned_question = preprocess_text(question)\n",
    "    documents = []\n",
    "    has_fetched_preimum_plans = False\n",
    "    has_fetched_total_investment_estimation = False\n",
    "    has_fetched_fund = False\n",
    "    has_fetched_premium_allocation = False\n",
    "    for keyword in re.findall(r'\\b\\w+\\b', cleaned_question):\n",
    "        match keyword:\n",
    "            case \"premium\" | \"plan\" | \"coverage\" | \"price\" | \"pricing\":\n",
    "                if not has_fetched_preimum_plans:\n",
    "                    print(\"fetching premium plans\")\n",
    "                    \n",
    "                    documents.append(list(db[\"premium_plans\"].find()))\n",
    "                    has_fetched_preimum_plans = True\n",
    "            case \"investment\" | \"value\" | \"allocation\":\n",
    "                if not has_fetched_total_investment_estimation:\n",
    "                    print(\"fetching investment plans\")\n",
    "                    \n",
    "                    documents.append(list(db[\"total_investment_estimations\"].find()))\n",
    "                    has_fetched_total_investment_estimation = True\n",
    "            case \"performance\" | \"perform\" | \"fund\":\n",
    "                if not has_fetched_fund:\n",
    "                    print(\"fetching fund performance\")\n",
    "                    \n",
    "                    documents.append(list(db[\"funds\"].find()))\n",
    "                    has_fetched_fund = True\n",
    "            case \"allocation\":\n",
    "                if not has_fetched_premium_allocation:\n",
    "                    print(\"fetching premium allocation\")\n",
    "                    \n",
    "                    documents.append(list(db[\"premium_allocations\"].find()))\n",
    "                    has_fetched_premium_allocation = True\n",
    "            case _:\n",
    "                continue\n",
    "\n",
    "    return documents\n",
    "# @tool\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def get_context(question: str, verbose: bool = False) -> list:\n",
    "    \"\"\"Retrieves text-based information for an insurance product only based on the user query.\n",
    "does not answer quwstions about the chat agent\"\"\"\n",
    "    print(question)\n",
    "    matches = semantic_search(question, verbose=verbose)\n",
    "    document_data = get_collection_matches(matches, verbose=verbose)\n",
    "    context = find_documents(document_data, verbose=verbose)\n",
    "    # if verbose:\n",
    "    #     for message in messages:\n",
    "    #         print(message)\n",
    "    template = ChatPromptTemplate.from_template(f\"\"\"fullfill the query with the provided information\n",
    "Query      :{{question}}\n",
    "Information:{{context}}\"\"\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = {\n",
    "        \"context\": lambda x: context , \"question\": RunnablePassthrough()\n",
    "        } | template | chatbot_model | StrOutputParser()\n",
    "    return chain.stream(question)\n",
    "\n",
    "# tools = [fetch_metrics,\n",
    "#         get_context\n",
    "#         ]\n",
    "# toolPicker = Ollama(model='llama3.2').bind_tools(tools)\n",
    "# for group in fetch_metrics(\"i need information on performing funds, how much my premium is allocated, the allocation rate, the pricing plan, how much will i be paying per premium payment, what is the coverage performance, what are the fundings\"):\n",
    "#     for document in group:\n",
    "#         print(document)\n",
    "\n",
    "# fetch_metrics(\"i need information on performing funds. how much my premium is allocated? the allocation rate, the pricing plan, how much will i be paying per premium payment, what is the coverage performance, what are the fundings, testing the word post-apocalyptic, 80 90% 20$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who manages this product\n",
      "<class 'langchain_pinecone.vectorstores.PineconeVectorStore'>\n",
      "[{'collection': 'general_info', 'id': '67286e4f987ce18efd0c3ec3'}]\n",
      "[{'collection': 'general_info', 'id': '67286e4f987ce18efd0c3ec3'}, {'collection': 'exclusions', 'id': '67286e50987ce18efd0c3ede'}]\n",
      "[{'collection': 'general_info', 'id': '67286e4f987ce18efd0c3ec3'}, {'collection': 'exclusions', 'id': '67286e50987ce18efd0c3ede'}, {'collection': 'exclusions', 'id': '67286e50987ce18efd0c3edf'}]\n",
      "[{'collection': 'general_info', 'id': '67286e4f987ce18efd0c3ec3'}, {'collection': 'exclusions', 'id': '67286e50987ce18efd0c3ede'}, {'collection': 'exclusions', 'id': '67286e50987ce18efd0c3edf'}, {'collection': 'requirements', 'id': '67286e50987ce18efd0c3ed1'}]\n",
      "[{'collection': 'general_info', 'id': '67286e4f987ce18efd0c3ec3'}, {'collection': 'exclusions', 'id': '67286e50987ce18efd0c3ede'}, {'collection': 'exclusions', 'id': '67286e50987ce18efd0c3edf'}, {'collection': 'requirements', 'id': '67286e50987ce18efd0c3ed1'}, {'collection': 'requirements', 'id': '67286e50987ce18efd0c3ece'}]\n",
      "[{'collection': 'general_info', 'id': '67286e4f987ce18efd0c3ec3'}, {'collection': 'exclusions', 'id': '67286e50987ce18efd0c3ede'}, {'collection': 'exclusions', 'id': '67286e50987ce18efd0c3edf'}, {'collection': 'requirements', 'id': '67286e50987ce18efd0c3ed1'}, {'collection': 'requirements', 'id': '67286e50987ce18efd0c3ece'}, {'collection': 'requirements', 'id': '67286e50987ce18efd0c3ecc'}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'generator'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m     chain \u001b[38;5;241m=\u001b[39m ({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: context , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough()}\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;241m|\u001b[39m template\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;241m|\u001b[39m chatbot_model\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;241m|\u001b[39m StrOutputParser())\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chain\u001b[38;5;241m.\u001b[39mstream(query)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mRAG\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwho manages this product\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 38\u001b[0m, in \u001b[0;36mRAG\u001b[0;34m(query, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m     template \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mfullfill the query with the provided information\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124mquery      :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124minformation:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# RAG pipeline\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     chain \u001b[38;5;241m=\u001b[39m (\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mRunnablePassthrough\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;241m|\u001b[39m chatbot_model\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;241m|\u001b[39m StrOutputParser())\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chain\u001b[38;5;241m.\u001b[39mstream(query)\n",
      "File \u001b[0;32m/Volumes/KINGSTON SNV2S1000G Media/prototypes/python/chatbot/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:589\u001b[0m, in \u001b[0;36mRunnable.__ror__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ror__\u001b[39m(\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    581\u001b[0m     other: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    586\u001b[0m     ],\n\u001b[1;32m    587\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RunnableSerializable[Other, Output]:\n\u001b[1;32m    588\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compose this Runnable with another object to create a RunnableSequence.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/Volumes/KINGSTON SNV2S1000G Media/prototypes/python/chatbot/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5837\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   5835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableLambda(cast(Callable[[Input], Output], thing))\n\u001b[1;32m   5836\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(thing, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m-> 5837\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Runnable[Input, Output], \u001b[43mRunnableParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthing\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   5838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5839\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   5840\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a Runnable, callable or dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5841\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5842\u001b[0m     )\n",
      "File \u001b[0;32m/Volumes/KINGSTON SNV2S1000G Media/prototypes/python/chatbot/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3539\u001b[0m, in \u001b[0;36mRunnableParallel.__init__\u001b[0;34m(self, steps__, **kwargs)\u001b[0m\n\u001b[1;32m   3536\u001b[0m merged \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msteps__} \u001b[38;5;28;01mif\u001b[39;00m steps__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   3537\u001b[0m merged\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   3538\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m-> 3539\u001b[0m     steps__\u001b[38;5;241m=\u001b[39m{key: \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, r \u001b[38;5;129;01min\u001b[39;00m merged\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   3540\u001b[0m )\n",
      "File \u001b[0;32m/Volumes/KINGSTON SNV2S1000G Media/prototypes/python/chatbot/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5843\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   5838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5839\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   5840\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a Runnable, callable or dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5841\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5842\u001b[0m     )\n\u001b[0;32m-> 5843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'generator'>"
     ]
    }
   ],
   "source": [
    "#setup the RAG workflow\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import ValidationError\n",
    "\n",
    "def RAG(query : str, verbose : bool | None = False):\n",
    "    if query in [\"exit\", \"quit\", \"bye\", \"end\", \"stop\", \"\"]:\n",
    "        return\n",
    "    # if verbose:\n",
    "    #     tool_query=query+\" (Turn on debug mode)\"\n",
    "    # else:\n",
    "    #     tool_query=query\n",
    "    # messages = [query]\n",
    "    \n",
    "    # ai_msg = toolPicker.invoke(tool_query)\n",
    "    # for tool_call in ai_msg.tool_calls:\n",
    "    #     selected_tool = {\"fetch_metrics\": fetch_metrics,\n",
    "    #                     \"get_context\": get_context\n",
    "    #                     }[tool_call[\"name\"].lower()]\n",
    "\n",
    "    #     tool_msg = None \n",
    "    #     try:\n",
    "    #         tool_msg = selected_tool.invoke(tool_call)\n",
    "    #     except ValidationError as e:\n",
    "    #         print(f\"Error: {e.json()}\")\n",
    "    #     print(f\"called {tool_call[\"name\"].lower()} with {tool_call['args']}\")\n",
    "        # messages.append(f\"connected {tool_call[\"name\"].lower()} function returned {tool_msg.content}\")\n",
    "    # messages.append(f\"connected {get_context(question=query, debug = verbose)} \")\n",
    "    context = get_context(question=query, verbose = verbose)\n",
    "\n",
    "    # if verbose:\n",
    "    #     for message in messages:\n",
    "    #         print(message)\n",
    "    template = ChatPromptTemplate.from_template(f\"\"\"fullfill the query with the provided information\n",
    "query      :{query}\n",
    "information:{context}\"\"\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = ({\"context\": context , \"question\": RunnablePassthrough()}\n",
    "        | template\n",
    "        | chatbot_model\n",
    "        | StrOutputParser())\n",
    "    return chain.stream(query)\n",
    "RAG(\"who manages this product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'normal'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup a simple intent classifier\n",
    "def classify_intent(user_input:str) -> str:\n",
    "    \"\"\"classify the intent of the user input\n",
    "\n",
    "current implementation uses a lightweight model (llama3.2:1B)\n",
    "with few-shot prompting examples\n",
    "for classifying 'normal', 'register', 'RAG' intents\n",
    "\n",
    "\"\"\"\n",
    "    return intent_classifier.invoke(f\"\"\"Classify the given input, use RAG if it is asking about insurance products,answer only with 'normal','register','RAG':\n",
    "\n",
    "example:\n",
    "\n",
    "Input: \"Is there a contact number\", Intent: RAG\n",
    "Input: \"How do I create a new account\", Intent: register\n",
    "Input: \"How do I make a claim\", Intent: RAG\n",
    "Input: \"Search the web for cat videos\", Intent: normal\n",
    "Input: \"Help me register for this service\", Intent: register\n",
    "Input: \"Where can I get started\", Intent: register\n",
    "Input: \"What is your name\", Intent: normal\n",
    "Input: \"What entities are attached to this service\", Intent: RAG\n",
    "Input: \"What is your purpose\", Intent: normal\n",
    "Input: \"Can I see some fund performance metrics\", Intent: RAG\n",
    "Input: \"What is the weather in your country\", Intent: normal\n",
    "Input: \"How can i register for an account\", Intent: register\n",
    "Input: \"Who owns the product\", Intent: RAG\n",
    "Input: \"Tell me how to subscribe\", Intent: register\n",
    "Input: \"Guide me through the registration process\", Intent: register\n",
    "Input: \"How do I sign up for the trial\", Intent: register\n",
    "Input: \"Can you explain your features\", Intent: normal\n",
    "Input: \"Sign me up\", Intent: register\n",
    "Input: \"Can you assist me with enrolling\", Intent: register\n",
    "Input: \"What services does the product provide\", Intent: RAG\n",
    "Input: \"Where can i wash my dog\", Intent: normal\n",
    "Input: \"Goodbye\", Intent: normal\n",
    "Input: \"How do i pay for the service\", Intent: RAG\n",
    "Input: \"how is my premium allocated\", Intent: RAG\n",
    "Input: \"Hello\", Intent: normal\n",
    "Input: \"What are the coverage options\", Intent: RAG\n",
    "Input: \"What's the first step to register\", Intent: register\n",
    "Input: \"What funds are involved\", Intent: RAG\n",
    "Input: \"Where are you located\", Intent: normal\n",
    "Input: \"Who do I contact for help\", Intent: RAG\n",
    "Input: \"Can I pay with a credit card\", Intent: RAG\n",
    "Input: \"Why should i trust you\", Intent: normal\n",
    "Input: \"What should i get ready for enrollment\", Intent: register\n",
    "Input: \"How are you\", Intent\": normal\n",
    "Input: \"What company distributes this service\", Intent: RAG\n",
    "Input: \"how do i know you are not a scam\", Intent: verify\n",
    "Input: \"Are there any additional charges\", Intent: RAG\n",
    "Input: \"What can you tell me about the available insurance plans\", Intent: RAG\n",
    "Input: \"how much do i need to pay for the insurance scheme\", Intent: RAG\n",
    "Input: \"tell me about the available insurance plans\", Intent: RAG\n",
    "Input: \"how did you get my number\", Intent: verify\n",
    "Input: \"show me verification so i know this isn't a scam\", Intent: verify\n",
    "Input: \"how do i know you are not scamming me\", Intent: verify\n",
    "Input: \"Why should I sign up for this plan\", Intent: RAG\n",
    "\n",
    "Input: {user_input}\"\"\").content\n",
    "\n",
    "classify_intent(\"who are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input: str):\n",
    "    intent = classify_intent(user_input)\n",
    "    print(intent)\n",
    "    if intent == \"RAG\":\n",
    "        return get_context(user_input)\n",
    "    elif intent == \"register\":\n",
    "        print(\"register function work in progress\")\n",
    "    else:\n",
    "        chain = chatbot_model | StrOutputParser()\n",
    "        return chain.stream(user_input)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n",
      "I don't have a personal name. I'm an AI designed to provide information and assist with tasks, but I don't have a personal identity or emotions. I exist solely to help users like you with their queries.\n",
      "\n",
      "If you'd like, I can suggest some alternative names that might be helpful in our conversation. For example, I could be referred to as \"Assistant\" or \"AI Companion.\" Let me know if there's anything else I can do to make our interaction more comfortable!"
     ]
    }
   ],
   "source": [
    "for chunk in chat(\"what is your name\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
