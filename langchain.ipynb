{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read: Store the data as vector embeddings in Atlas\n",
    "# from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# searchDictionaries = dict()\n",
    "# for collection_name in collection_names:\n",
    "#     # collection = db[collection_name]\n",
    "#     # vector_search = MongoDBAtlasVectorSearch(\n",
    "#     #     collection=collection,\n",
    "#     #     embedding=model,\n",
    "#     #     index_name=\"vector_index\"\n",
    "#     #     )\n",
    "#     #Create: store the data as vector embeddings in Atlas\n",
    "#     # vector_search = MongoDBAtlasVectorSearch.from_documents(\n",
    "#     #     documents = docs,\n",
    "#     #     embedding = model,\n",
    "#     #     collection = collection,\n",
    "#     #     index_name = \"vector_index\"\n",
    "#     # )\n",
    "#     # Create a Pinecone index\n",
    "#     vector_search = pc.create_index(\n",
    "#       name=collection_name,\n",
    "#       dimension=768,\n",
    "#       metric='cosine',\n",
    "#       spec=ServerlessSpec(\n",
    "#           cloud='aws',\n",
    "#           region='us-west-2'\n",
    "#       )\n",
    "# )\n",
    "#     searchDictionaries[collection_name] = vector_search\n",
    "    \n",
    "# searchDictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymongo.operations import SearchIndexModel\n",
    "# # Create your index model, then create the search index\n",
    "\n",
    "# for collection_name, searchDictionaries in searchDictionaries.items():\n",
    "#   print(f'''\n",
    "#         creating index for {collection_name}''')\n",
    "  #if using mongoDB\n",
    "    # collection = db[collection_name]\n",
    "    # search_index_model = SearchIndexModel(\n",
    "    #     definition = {\n",
    "    #         \"fields\": [\n",
    "    #             {\n",
    "    #                 \"type\": \"vector\",\n",
    "    #                 \"numDimensions\": 768,\n",
    "    #                 \"path\": \"embedding\",\n",
    "    #                 \"similarity\": \"cosine\"\n",
    "    #             }\n",
    "    #         ]\n",
    "    #     },\n",
    "    #     name = \"vector_index\",\n",
    "    #     type = \"vectorSearch\"\n",
    "    # )\n",
    "    # collection.create_search_index(model=search_index_model)\n",
    "\n",
    "  #if using pinecone\n",
    "  #Create index\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# import os\n",
    "\n",
    "# # Authenticate to your Hugging Face account\n",
    "# os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# # Access the LLM (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    "# llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# # Create prompt and RAG workflow\n",
    "# prompt = PromptTemplate.from_template(\"\"\"\n",
    "#    Answer the following question based on the given context.\n",
    "\n",
    "#    Question: {question}\n",
    "#    Context: {context}\n",
    "# \"\"\")\n",
    "\n",
    "# rag_chain = (\n",
    "#    { \"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "#    | prompt\n",
    "#    | llm\n",
    "#    | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# # Prompt the LLM\n",
    "# question = \"can you list out all the covered ilnesses?\"\n",
    "# answer = rag_chain.invoke(question)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate Atlas Vector Search as a retriever\n",
    "# retriever = vector_search.as_retriever(\n",
    "#    search_type = \"similarity\"\n",
    "# )\n",
    "# # Run a sample query in order of relevance\n",
    "# retriever.invoke(\"illnesses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "# %pip install -qU pymongo langchain langchain_community langchain_mongodb langchain_huggingface nltk langchain-ollama langchain-pinecone pinecone-notebooks einops langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assuming you have a JSON copy of the data   \n",
    "Data aquisition:   \n",
    "import the json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json files\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "html_path = './GSS315-SIO-Brochure-v2.html'\n",
    "json_path = './data.json'\n",
    "# loader = UnstructuredHTMLLoader(html_path)\n",
    "# loader = BSHTMLLoader(html_path, open_encoding='latin-1')\n",
    "# load a json file as data\n",
    "data  = json.loads(Path(json_path).read_text())\n",
    "html_path = None\n",
    "json_path = None\n",
    "del html_path\n",
    "del json_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data ingestion:   \n",
    "since it is better to supply the full context of structured and unstructured data fetched in the future,   \n",
    "we will store the raw text in mongodb first   \n",
    "and apply preprocessing later when we need semantically/meaning accurate search results (important for vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to your MongoDB Atlas(Cloud) cluster\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb+srv://damonngkhaiweng:jhT8LGE0qi6XsKfz@chatbotcluster.noyfa.mongodb.net/?retryWrites=true&w=majority&appName=chatbotcluster\")\n",
    "db = client[\"product1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_targets already exists in the database, skipping insertion\n",
      "insertion complete\n"
     ]
    }
   ],
   "source": [
    "#specify outer level json objects as mongodb collection names, then based on the specified names, create new collections and insert corresponding data\n",
    "\n",
    "#specify all the collections\n",
    "collection_names = []\n",
    "for key, value in data.items():\n",
    "    collection_names.append(key)\n",
    "    # if type(value) != str:\n",
    "    #     value = json.dumps(value)\n",
    "    # print(key+\":    \"+value)\n",
    "    \n",
    "#fetch existing collections\n",
    "ignored_and_existing_collections = db.list_collection_names()\n",
    "\n",
    "#specify the collections to ignore\n",
    "if \"questions_and_answers2\" not in ignored_and_existing_collections:\n",
    "        ignored_and_existing_collections.append(\"questions_and_answers2\")\n",
    "if \"exclusions2\" not in ignored_and_existing_collections:\n",
    "        ignored_and_existing_collections.append(\"exclusions2\")\n",
    "\n",
    "for collection_name in collection_names:\n",
    "    if collection_name in ignored_and_existing_collections:\n",
    "       continue\n",
    "    collection = db[collection_name]\n",
    "    if isinstance(data[collection_name], list):\n",
    "        print(f\"\"\"\n",
    "inserting many records of {data[collection_name]} into {collection_name} collection\n",
    "\"\"\")\n",
    "        collection.insert_many(data[collection_name])\n",
    "    else:\n",
    "        print(f\"\"\"\n",
    "inserting one record of {data[collection_name]} into {collection_name} collection\n",
    "\"\"\")\n",
    "        collection.insert_one(data[collection_name])\n",
    "\n",
    "# separately insert the vector targets\n",
    "collection = db[\"vector_targets\"]\n",
    "vector_target_records = list(collection.find())\n",
    "if len(vector_target_records) == 0:\n",
    "        print(f\"\"\"\n",
    "inserting one record of {data[\"vector_targets\"]} into {collection_name} collection\n",
    "\"\"\")\n",
    "        collection.insert_many(data[\"vector_targets\"])        \n",
    "else:\n",
    "    print(\"vector_targets already exists in the database, skipping insertion\")\n",
    "\n",
    "print(\"insertion complete\")\n",
    "\n",
    "ignored_and_existing_collections = None\n",
    "del ignored_and_existing_collections\n",
    "collection_names = None\n",
    "del collection_names\n",
    "vector_target_records = None\n",
    "del vector_target_records\n",
    "data = None\n",
    "del data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further preprocessing:   \n",
    "lowercasing   \n",
    "special character removal\n",
    "\n",
    "stop words removal\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/damonng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#use nltk to lowercase, remove special characters, tokenize, remove stopwords, and lemmatize the values in the json data\n",
    "#fetch all data cleaning resources\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#initialize the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#allow specific stopwords\n",
    "stop_words.remove(\"both\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    pattern = re.compile(r\"[;@#&*!()\\[\\]]\")\n",
    "    def get_wordnet_pos(tag):\n",
    "        match tag[0]:\n",
    "            case 'J':\n",
    "                return wordnet.ADJ\n",
    "            case 'V':\n",
    "                return wordnet.VERB\n",
    "            case 'R':\n",
    "                return wordnet.ADV\n",
    "            case _:\n",
    "                return wordnet.NOUN\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # Remove stop words and lemmatize\n",
    "    processed_tokens = [\n",
    "        lemmatizer.lemmatize(word.lower(), get_wordnet_pos(tag)) \n",
    "        for word, tag in pos_tags \n",
    "        if word.lower() not in stop_words and not pattern.match(word)\n",
    "    ]\n",
    "    # Join tokens back to a string\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "#setup a function to recursively preprocess the json data\n",
    "def traverse_json(data, json_text_action, target_key=None):\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, str) and (target_key is None or key == target_key):\n",
    "                # Preprocess the string value\n",
    "                data[key] = json_text_action(value)\n",
    "            elif isinstance(value, dict) or isinstance(value, list):\n",
    "                # Recursively preprocess the dictionary or list\n",
    "                traverse_json(value, json_text_action)\n",
    "    elif isinstance(data, list):\n",
    "        for i, element in enumerate(data):\n",
    "            if isinstance(element, str):\n",
    "                # Preprocess the string in the list and save it in the original position\n",
    "                data[i] = json_text_action(element)\n",
    "            elif isinstance(element, dict) or isinstance(element, list):\n",
    "                # Recursively preprocess the dictionary or list\n",
    "                traverse_json(element, json_text_action)\n",
    "    return data\n",
    "\n",
    "# preprocess_text(\"i need information on performing funds. how much my premium is allocated? the allocation rate, the pricing plan, how much will i be paying per premium payment, what is the coverage performance, what are the fundings, testing the word post-apocalyptic, 80 90% 20$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if necessary, chunk the longer text I.E. descriptions, notes, bullet points, disclaimers, etc\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
    "# docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/KINGSTON SNV2S1000G Media/prototypes/python/dataprocessing/.venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Connect to your pinecone vector database and specify the working index\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone(\"ce2e7e04-18d6-4408-a9ab-7527162af1d7\")\n",
    "index = pc.Index(\"product1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data embedding//vectorization\n",
    "\n",
    "target descriptive values and generate embeddings for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model (https://huggingface.co/nomic-ai/nomic-embed-text-v1.5\")\n",
    "# %pip install sentence-transformers scipy\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# model = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1.5\", model_kwargs={ \"trust_remote_code\": True })\n",
    "\n",
    "global_embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "\n",
    "number_of_dimensions = 768 #the embedding dimensions according to the model documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the target fields to embed\n",
    "from copy import deepcopy\n",
    "\n",
    "def vectorize(texts):\n",
    "    \"vectorize the provided texts using a globally declared model.\"    \n",
    "    vectors = global_embedding_model.encode(texts)\n",
    "    return vectors\n",
    "\n",
    "ignore_collections = [\"vector_targets\", \"premium_plans\",\"important_notice\",\"total_investment_estimations\",\"premium_allocations\",\"test2\", \"test\",\"questions_and_answers2\",\"exclusions2\"]\n",
    "\n",
    "def vectorize_and_index(mongo_database=db, vector_index=index, ignore_collections=ignore_collections, debug=False):\n",
    "    \"\"\"vectorize the target fields and store the embeddings in the pinecone index.\n",
    "    \n",
    "current implmentation does not support converting mongo collection name to pinecone namespaces.\"\"\"\n",
    "    if mongo_database == None:\n",
    "        raise ValueError(\"mongo_database is required, the mongo database should also have a vector_targets collection specifying the target fields to vectorize\")\n",
    "    if not vector_index:\n",
    "        raise ValueError(\"vector_index is required\")\n",
    "    vectors = []\n",
    "    for collection_name in mongo_database.list_collection_names():\n",
    "        if collection_name in ignore_collections:\n",
    "            continue                                            #skip collections that should not to be vectorized\n",
    "        if debug:\n",
    "            print(f\"In {collection_name}\")\n",
    "        results = list(mongo_database[collection_name].find())\n",
    "        targets = list(mongo_database.vector_targets.find({\"name\": collection_name}))\n",
    "        for record in results:\n",
    "            if targets:\n",
    "                for target in targets:                              #iterate through the target fields from the same collection\n",
    "                    old_data = deepcopy(record[target['field']])              #store original info\n",
    "                    traverse_json(record, preprocess_text, target['field'])\n",
    "                    cleaned_data = record[target['field']]                     #clean the data\n",
    "                    vector = vectorize(record[target['field']])                #store the vectorized embeddings\n",
    "                    if debug:\n",
    "                        print(f\"original: {old_data}\\ncleaned : {cleaned_data}\\nvector  : {len(vector)}\")\n",
    "                    if len(vector) == number_of_dimensions: #check if the vectorized embeddings are of the correct length\n",
    "                        vectors.append({\"id\": str(record['_id'])+\"-\"+collection_name+\"-\"+target['field'], \n",
    "                                        \"values\": vector\n",
    "                                        ,\"metadata\": {\"original\":cleaned_data}\n",
    "                                        })\n",
    "                        pass\n",
    "                    else:                                           \n",
    "                        #if the vectorized embeddings length not correct, possibility of other datatypes, iterate through the iterable\n",
    "                        for j , sub_vector in enumerate(vector):\n",
    "                            if debug:\n",
    "                                print(len(sub_vector),cleaned_data[j])\n",
    "                            vectors.append({\"id\": str(record['_id'])+\"-\"+collection_name+\"-\"+target['field']+\"-\"+str(j), \n",
    "                                            \"values\": sub_vector\n",
    "                                            ,\"metadata\": {\"original\":cleaned_data[j]}\n",
    "                                            })\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(\"no matching target, skipping vectorization\")\n",
    "                continue\n",
    "            \n",
    "    vector_index.upsert(vectors=vectors)\n",
    "\n",
    "# vectorize_and_index(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using local(ollama) base model as chatbot model\n",
    "from langchain_ollama.chat_models import ChatOllama as Ollama\n",
    "\n",
    "chatbot_model = Ollama(model='llama3.2', request_timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'read_units': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'matches': [{'id': '67171ded2ca774a8bfd83375-questions_and_answers-answer-2',\n",
       "               'score': 0.499227405,\n",
       "               'values': []},\n",
       "              {'id': '67171ded2ca774a8bfd83372-questions_and_answers-answer',\n",
       "               'score': 0.469530821,\n",
       "               'values': []},\n",
       "              {'id': '67171dec2ca774a8bfd8334d-advantages-advantage',\n",
       "               'score': 0.464928329,\n",
       "               'values': []},\n",
       "              {'id': '6717364e2ca774a8bfd83399-funds-description-2',\n",
       "               'score': 0.463731647,\n",
       "               'values': []},\n",
       "              {'id': '67171dec2ca774a8bfd8334b-critical_illnesses-disclaimer',\n",
       "               'score': 0.460437477,\n",
       "               'values': []},\n",
       "              {'id': '67171ded2ca774a8bfd83352-requirements-description',\n",
       "               'score': 0.454422921,\n",
       "               'values': []}],\n",
       "  'namespace': '',\n",
       "  'usage': {'read_units': 5}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup the semantic search function\n",
    "def semantic_search(question, model=global_embedding_model, index=index, top_k=6, debug=False):\n",
    "    \"\"\"vectorizes the question and queries the pinecone index for the top 3 closest matches.\n",
    "\n",
    "current implementation does not support querying multiple namespaces or using mongoDB indexes.\n",
    "\n",
    "Args:\n",
    "    question (str): _description_\n",
    "    debug (bool, optional): _description_. Defaults to False.\n",
    "    model (SentenceTransformer, optional): _description_. Defaults to global_embedding_model.\n",
    "    index (Pinecone.Index, optional): _description_. Defaults to index.\n",
    "\n",
    "Returns:\n",
    "    list(QueryResponse): the top closest query results from the pinecone index\n",
    "    \"\"\"\n",
    "    if not index:\n",
    "        print(\"Index not found, please specify your pinecone or mongo search index\")\n",
    "        return\n",
    "    if not model:\n",
    "        print(\"Model not found\")\n",
    "        return\n",
    "    if debug:\n",
    "        print(\"Encoding question...\")\n",
    "        \n",
    "    vectors = model.encode(question)\n",
    "    \n",
    "    if not isinstance(vectors, list): # Ensure vectors is a list of floats\n",
    "        vectors = vectors.tolist()\n",
    "    matches = []\n",
    "    spaces = index.describe_index_stats()['namespaces']\n",
    "    for key, value in spaces.items():\n",
    "        res = index.query(\n",
    "            namespace=key,\n",
    "            vector=vectors,\n",
    "            top_k=top_k,\n",
    "            include_metadata=False,\n",
    "            include_values=False\n",
    "        )\n",
    "        matches.append(res)\n",
    "        print(res['usage'])\n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "response = semantic_search(\"who manages this product\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting collection matches...\n",
      "no pinecone namespace found, checking id or additional metadata\n",
      "{'id': '67171ded2ca774a8bfd83375-questions_and_answers-answer-2',\n",
      " 'score': 0.499227405,\n",
      " 'values': []}\n",
      "{'id': '67171ded2ca774a8bfd83372-questions_and_answers-answer',\n",
      " 'score': 0.469530821,\n",
      " 'values': []}\n",
      "{'id': '67171dec2ca774a8bfd8334d-advantages-advantage',\n",
      " 'score': 0.464928329,\n",
      " 'values': []}\n",
      "{'id': '6717364e2ca774a8bfd83399-funds-description-2',\n",
      " 'score': 0.463731647,\n",
      " 'values': []}\n",
      "{'id': '67171dec2ca774a8bfd8334b-critical_illnesses-disclaimer',\n",
      " 'score': 0.460437477,\n",
      " 'values': []}\n",
      "{'id': '67171ded2ca774a8bfd83352-requirements-description',\n",
      " 'score': 0.454422921,\n",
      " 'values': []}\n"
     ]
    }
   ],
   "source": [
    "#setup a function to receive semantic search results and return the collection name and ids\n",
    "def get_collection_matches(response, debug=False):\n",
    "    if debug:\n",
    "        print(\"Getting collection matches...\")\n",
    "    document_metadata = []\n",
    "    for collection_matches in response:\n",
    "        collection = collection_matches['namespace']\n",
    "        is_using_default_namespace = False\n",
    "        if collection == \"\":\n",
    "            is_using_default_namespace = True\n",
    "            if debug:\n",
    "                print(\"no pinecone namespace found, checking id or additional metadata\")\n",
    "        ids = []\n",
    "        for match in collection_matches['matches']:\n",
    "            if debug:\n",
    "                print(match)\n",
    "            data_from_id = match['id'].split(\"-\")\n",
    "            match_id = data_from_id[0]\n",
    "            if is_using_default_namespace:\n",
    "                collection = data_from_id[1]\n",
    "            #filter out duplicate ids\n",
    "            if match_id not in ids:\n",
    "                ids.append(match_id)\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"Duplicate id {match_id} for {collection} found in fetch list, ignoring\")\n",
    "                continue\n",
    "            if is_using_default_namespace:\n",
    "                document_metadata.append({'collection':collection, 'id':match_id})\n",
    "        if not is_using_default_namespace:\n",
    "            document_metadata.append({'collection':collection, 'ids':ids})\n",
    "        \n",
    "    return document_metadata\n",
    "\n",
    "document_metadata = get_collection_matches(response, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('67171ded2ca774a8bfd83375'),\n",
       "  'question': 'What are the current fees and charges?',\n",
       "  'answer': ['Insurance charges are Applicable to the sum assured, and vary depending on the average age profile and claim experience of the scheme.',\n",
       "   'Monthly Policy Fee is RM5.00',\n",
       "   'Fund Management Charge is 0.50% per annum. Note: The fees and charges levied may change from time to time.']},\n",
       " {'_id': ObjectId('67171ded2ca774a8bfd83372'),\n",
       "  'question': 'Can the Assured Member/spouse/children apply to contribute more',\n",
       "  'answer': 'Yes. The Assured Member/spouse/children will be required to reapply by completing a standard Group Proposal form, subject to approval by the Company and up to the maximum benefit allowed'},\n",
       " {'_id': ObjectId('67171dec2ca774a8bfd8334d'),\n",
       "  'advantage': 'Automatic premium remittance via credit card / bank deduction / JomPay ensures continuous protection.'},\n",
       " {'_id': ObjectId('6717364e2ca774a8bfd83399'),\n",
       "  'name': 'Dana Gemilang',\n",
       "  'description': ['The Dana Gemilang is a fund where 80% to 100% of the investments are in equities, which may be volatile in the short term.',\n",
       "   \"This fund seeks to achieve medium to long-term capital appreciation. Although the fund invests mainly in Malaysia (50% to 100%), it may also partially invest in Singapore (up to 25%) and Hong Kong (up to 25%), if and when necessary, to enhance the fund's returns.\",\n",
       "   'The fund only invests in Shariah-approved securities. Although this fund invests in Shariah-approved securities, the investment-linked insurance plan that utilises this fund is not a Shariah-compliant product.'],\n",
       "  'fund_performance': [{'year': 2017, 'return_percentage': 13.95},\n",
       "   {'year': 2018, 'return_percentage': -17.84},\n",
       "   {'year': 2019, 'return_percentage': 4.14},\n",
       "   {'year': 2020, 'return_percentage': 1.77},\n",
       "   {'year': 2021, 'return_percentage': -2.52}]},\n",
       " {'_id': ObjectId('67171dec2ca774a8bfd8334b'),\n",
       "  'disclaimer': 'Upon payment of claim for Angioplasty and other invasive treatments for coronary artery disease, the sum assured will be reduced by the quantum of the payment for Angioplasty and other invasive treatments for coronary artery disease. However, the premium shall remain unchanged. This benefit is subject to a limit of RM1,000,000 under the Policy and all other non-credit-related group policies (including supplementary contract and endorsement, if any) issued by the Company by any name or description which provide Critical Illness benefits or similar benefits on the same Life Assured. Complete definition of Critical Illness as mentioned in the Master Policy need to be fulfilled before any 45 Critical Illnesses claim can become payable.',\n",
       "  'illnesses': ['Heart Attack - of specified severity',\n",
       "   'Stroke - resulting in permanent neurological deficit',\n",
       "   'Coronary Artery By-pass Surgery',\n",
       "   'Cancer - of specified severity',\n",
       "   'Kidney Failure - requiring dialysis or transplant',\n",
       "   'Fulminant Viral Hepatitis',\n",
       "   'Major Organ/Bone Marrow Transplant',\n",
       "   'Paralysis of Limbs',\n",
       "   'Multiple Sclerosis',\n",
       "   'Primary Pulmonary Arterial Hypertension',\n",
       "   'Blindness - Permanent and Irreversible',\n",
       "   'Heart Valve Surgery',\n",
       "   'Deafness - Permanent and Irreversible',\n",
       "   'Surgery to Aorta',\n",
       "   'Loss of Speech',\n",
       "   \"Alzheimer's Disease / Severe Dementia\",\n",
       "   'Third Degree Burns - of specified severity',\n",
       "   'Coma - resulting in permanent neurological deficit',\n",
       "   'Cardiomyopathy - of specified severity',\n",
       "   'Motor Neuron Disease',\n",
       "   'HIV Infection due to Blood Transfusion',\n",
       "   \"Parkinson's Disease\",\n",
       "   'End-Stage Liver Failure',\n",
       "   'End-Stage Lung Disease',\n",
       "   'Major Head Trauma',\n",
       "   'Chronic Aplastic Anemia',\n",
       "   'Muscular Dystrophy',\n",
       "   'Benign Brain Tumor',\n",
       "   'Encephalitis',\n",
       "   'Angioplasty and other Invasive Treatments for Coronary Artery Disease',\n",
       "   'Brain Surgery',\n",
       "   'Bacterial Meningitis',\n",
       "   'Serious Coronary Artery Disease',\n",
       "   'Loss of Independent Existence',\n",
       "   'Systemic Lupus Erythematosus with Severe Kidney Complications',\n",
       "   'Full-blown AIDS',\n",
       "   'Medullary Cystic Disease',\n",
       "   'Occupationally Acquired HIV',\n",
       "   'Terminal Illness',\n",
       "   'Apallic Syndrome',\n",
       "   'Poliomyelitis',\n",
       "   'Progressive Scleroderma',\n",
       "   'Chronic Relapsing Pancreatitis',\n",
       "   'Elephantiasis',\n",
       "   'Creutzfeldt-Jakob Disease']},\n",
       " {'_id': ObjectId('67171ded2ca774a8bfd83352'),\n",
       "  'demographic': 'general',\n",
       "  'description': 'Assured Members (employees/members) and their legal spouse aged between nineteen (19) to sixty (60) years next birthday.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the mongo documents based on their collection and ids\n",
    "from bson.objectid import ObjectId as oid \n",
    "def find_documents(collection_matches, debug=False, database=db):\n",
    "    if debug:\n",
    "        print(\"Finding documents...\")\n",
    "    documents = []\n",
    "    for collection_match in collection_matches:\n",
    "        collection = database[collection_match['collection']]\n",
    "        if 'ids' in collection_match:\n",
    "            for id in collection_match['ids']:\n",
    "                documents.append(collection.find_one({\"_id\": oid(id)}))\n",
    "        else:\n",
    "            documents.append(collection.find_one({\"_id\": oid(collection_match['id'])}))\n",
    "    return documents\n",
    "\n",
    "documents = find_documents(document_metadata)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the previous 3 functions only allow access to unstructured data, but ignores structured data like tables   \n",
    "add a component for fetching structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the chatbot model for tool use\n",
    "from langchain_core.tools import tool\n",
    "import re\n",
    "\n",
    "@tool\n",
    "def fetch_metrics(question: str) -> list:\n",
    "    \"\"\"Retrieves only JSON of table data for either premium plans, total investment estimations, premium allocation or fund performance based on the user query.\"\"\"\n",
    "    cleaned_question = preprocess_text(question)\n",
    "    documents = []\n",
    "    has_fetched_preimum_plans = False\n",
    "    has_fetched_total_investment_estimation = False\n",
    "    has_fetched_fund = False\n",
    "    has_fetched_premium_allocation = False\n",
    "    for keyword in re.findall(r'\\b\\w+\\b', cleaned_question):\n",
    "        match keyword:\n",
    "            case \"premium\" | \"plan\" | \"coverage\" | \"price\" | \"pricing\":\n",
    "                if not has_fetched_preimum_plans:\n",
    "                    print(\"fetching premium plans\")\n",
    "                    \n",
    "                    documents.append(list(db[\"premium_plans\"].find()))\n",
    "                    has_fetched_preimum_plans = True\n",
    "            case \"investment\" | \"value\" | \"allocation\":\n",
    "                if not has_fetched_total_investment_estimation:\n",
    "                    print(\"fetching investment plans\")\n",
    "                    \n",
    "                    documents.append(list(db[\"total_investment_estimations\"].find()))\n",
    "                    has_fetched_total_investment_estimation = True\n",
    "            case \"performance\" | \"perform\" | \"fund\":\n",
    "                if not has_fetched_fund:\n",
    "                    print(\"fetching fund performance\")\n",
    "                    \n",
    "                    documents.append(list(db[\"funds\"].find()))\n",
    "                    has_fetched_fund = True\n",
    "            case \"allocation\":\n",
    "                if not has_fetched_premium_allocation:\n",
    "                    print(\"fetching premium allocation\")\n",
    "                    \n",
    "                    documents.append(list(db[\"premium_allocations\"].find()))\n",
    "                    has_fetched_premium_allocation = True\n",
    "            case _:\n",
    "                continue\n",
    "\n",
    "    return documents\n",
    "\n",
    "@tool\n",
    "def get_context(question: str, debug: bool = False) -> list:\n",
    "    \"\"\"Retrieves text-based information for an insurance product only based on the user query.\n",
    "does not answer quwstions about the chat agent\"\"\"\n",
    "    print(question)\n",
    "    matches = semantic_search(question, debug=debug)\n",
    "    document_data = get_collection_matches(matches, debug=debug)\n",
    "    context = find_documents(document_data, debug=debug)\n",
    "    return context\n",
    "\n",
    "tools = [fetch_metrics,\n",
    "        get_context\n",
    "        ]\n",
    "toolPicker = Ollama(model='llama3.2').bind_tools(tools)\n",
    "# for group in fetch_metrics(\"i need information on performing funds, how much my premium is allocated, the allocation rate, the pricing plan, how much will i be paying per premium payment, what is the coverage performance, what are the fundings\"):\n",
    "#     for document in group:\n",
    "#         print(document)\n",
    "\n",
    "# fetch_metrics(\"i need information on performing funds. how much my premium is allocated? the allocation rate, the pricing plan, how much will i be paying per premium payment, what is the coverage performance, what are the fundings, testing the word post-apocalyptic, 80 90% 20$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/KINGSTON SNV2S1000G Media/prototypes/python/dataprocessing/.venv/lib/python3.12/site-packages/langsmith/client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#setup the chat function with RAG workflow\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain import hub\n",
    "import pprint\n",
    "prompt = ChatPromptTemplate.from_messages([\"read the provided info from the database and answer question: {question}:\\n{context}\"])\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(\"what is the weather in {city}?\")\n",
    "# parser = StrOutputParser()\n",
    "\n",
    "# chain = prompt | chatbot_model | parser\n",
    "# # resp = chatbot_model.ainvoke(prompt, )\n",
    "\n",
    "def chat(query, debug=False):\n",
    "    if query in [\"exit\", \"quit\", \"bye\", \"end\", \"stop\", \"\"]:\n",
    "        return\n",
    "    if debug:\n",
    "        tool_query=query+\" (Turn on debug mode)\"\n",
    "    else:\n",
    "        tool_query=query\n",
    "    messages = [query]\n",
    "    agent = create_react_agent(toolPicker, tools)\n",
    "    chunks = []\n",
    "    for chunk in agent.stream({\"input\": tool_query}):\n",
    "        if \"actions\" in chunk:\n",
    "            for action in chunk[\"actions\"]:\n",
    "                print(f\"Calling Tool: `{action.tool}` with input `{action.tool_input}`\")\n",
    "        # Observation\n",
    "        elif \"steps\" in chunk:\n",
    "            for step in chunk[\"steps\"]:\n",
    "                print(f\"Tool Result: `{step.observation}`\")\n",
    "        # Final result\n",
    "        elif \"messages\" in chunk:\n",
    "            for message in chunk[\"messages\"]:\n",
    "                print(message.content, end='', flush=True)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        print(\"---\")\n",
    "    # ai_msg = toolPicker.invoke(tool_query)\n",
    "    \n",
    "    # for tool_call in ai_msg.tool_calls:\n",
    "    #     selected_tool = {\"fetch_metrics\": fetch_metrics,\n",
    "    #                     \"get_context\": get_context\n",
    "    #                     }[tool_call[\"name\"].lower()]\n",
    "\n",
    "    #     tool_msg = selected_tool.invoke(tool_call)\n",
    "    #     print(f\"called {tool_call['args']}\")\n",
    "    #     messages.append(f\"connected {tool_call[\"name\"].lower()} function returned {tool_msg.content}\")\n",
    "\n",
    "    # if debug:\n",
    "    #     for message in messages:\n",
    "    #         print(message)\n",
    "    # for chunk in agent_executor.stream(messages):\n",
    "    #     content = chunk.content\n",
    "    #     print(content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Tool: `get_context` with input `{'debug': False, 'question': 'What is your purpose'}`\n",
      "---\n",
      "What is your purpose\n",
      "{'read_units': 5}\n",
      "Tool Result: `[{'_id': ObjectId('67171ded2ca774a8bfd83372'), 'question': 'Can the Assured Member/spouse/children apply to contribute more', 'answer': 'Yes. The Assured Member/spouse/children will be required to reapply by completing a standard Group Proposal form, subject to approval by the Company and up to the maximum benefit allowed'}, {'_id': ObjectId('67171ded2ca774a8bfd8336f'), 'title': 'total and permanent disability or partial and permanent disability', 'descriptions': ['Resulted from self-inflicted injuries while sane or insane.', 'Sustained as a result of any form of flying except as a passenger on a regular scheduled flight.', 'Existed before the effective date of the coverage under the plan.', 'Resulted from war, whether declared or not declared.', 'Resulted from Life Assured driving a motor vehicle without possessing a valid driving licence. This exclusion will not apply if the Life Assured has an expired driving licence but is not disqualified from holding or obtaining such driving licence under any laws, by-laws or regulations', 'Resulted from provoked assault, drugs, scuba-diving, any form of racing (other than on foot).', 'In the event of sustaining disability after age sixty-five (65) years next birthday.']}, {'_id': ObjectId('67171ded2ca774a8bfd83371'), 'question': 'Can the spouse/children continue to participate in this scheme if the Assured Member dead/ disabled/ contracted one of the 45 Critical Illnesses before attaining age sixty-five (65) years age next birthday', 'answer': 'Yes. They can participate until they reach the maximum expiry age of sixty-five (65) years age next birthday.'}, {'_id': ObjectId('67171ded2ca774a8bfd83362'), 'title': 'Retirement Fund', 'description': 'Upon attaining age sixty-five (65) years next birthday, the Life Assured is eligible to redeem all his units available under the Dana Gemilang and the redemption value will be based on the net asset value, provided that the Life Assured has not made any claim, which resulted in the termination of this assurance.'}]`\n",
      "---\n",
      "My purpose is to assist users with information related to the Assured Member/spouse/children's benefits under a life insurance policy. I can provide answers to questions such as:\n",
      "\n",
      "* Can the Assured Member/spouse/children apply to contribute more?\n",
      "* What are the total and permanent disability or partial and permanent disability benefits?\n",
      "* Can the spouse/children continue to participate in this scheme if the Assured Member is dead/disabled/contracted one of the 45 Critical Illnesses before attaining age sixty-five (65) years age next birthday?\n",
      "* What is the Retirement Fund benefit?\n",
      "\n",
      "I can provide information on these topics and others related to the life insurance policy's benefits and conditions.---\n"
     ]
    }
   ],
   "source": [
    "chat(\"what is your purpose\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
